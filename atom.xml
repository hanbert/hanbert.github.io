<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一本正经的小马</title>
  <subtitle>假装不正经</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.hanbert.cn/"/>
  <updated>2017-04-10T01:06:14.745Z</updated>
  <id>https://www.hanbert.cn/</id>
  
  <author>
    <name>小马</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kolla部署工具简介</title>
    <link href="https://www.hanbert.cn/2017/04/10/Kolla%E9%83%A8%E7%BD%B2%E5%B7%A5%E5%85%B7%E7%AE%80%E4%BB%8B/"/>
    <id>https://www.hanbert.cn/2017/04/10/Kolla部署工具简介/</id>
    <published>2017-04-10T01:02:37.000Z</published>
    <updated>2017-04-10T01:06:14.745Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h3><p>Kolla项目是2014年9月份，Steven Dake提交的，这位老兄以前是HeatPTL，还是Corosync作者，牛的一塌糊涂。对于OpenStack的项目是非常熟悉，并且以前是红帽工程师，目前跳槽到思科，代表思科推出Kolla项目。</p>
<a id="more"></a>
<p>Kolla的目标，就是要做到100个节点开箱即用，所有的组件的HA都具备。简单说，Fuel装完是什么，他就是什么样子。实现的代价肯定比Fuel小很多。</p>
<p>Kolla，就是把目前OpenStack项目用到的所有组件都容器化。</p>
<p><img src="http://www.chenshake.com/wp-content/uploads/2016/06/kolla.jpg" alt="架构图"></p>
<p>除了上面之外，还包括一下内容：</p>
<ul>
<li>libvirt</li>
<li>qemu</li>
<li>OVS 和linux bridge</li>
<li>Ceph</li>
<li>HAproxy，Keeplived</li>
<li>MariaDB</li>
<li>ELK ( Heka )</li>
<li>MongoDB</li>
<li>rabbitmq</li>
</ul>
<p>Kolla 会对OpenStack下面的50多个项目进行build镜像，如果能够全部完成，那么就能够实现OpenStack的全面容器化。<br><strong>目前已经支持的组件镜像有：</strong></p>
<ul>
<li>Aodh</li>
<li>Barbican</li>
<li>Bifrost</li>
<li>Cinder</li>
<li>CloudKitty</li>
<li>Congress</li>
<li>Designate</li>
<li>Freezer</li>
<li>Glance</li>
<li>Gnocchi</li>
<li>Heat</li>
<li>Horizon</li>
<li>Ironic</li>
<li>Karbor</li>
<li>Keystone</li>
<li>Kuryr</li>
<li>Magnum</li>
<li>Manila</li>
<li>Mistral</li>
<li>Monasca</li>
<li>Murano</li>
<li>Neutron</li>
<li>Nova</li>
<li>Octavia</li>
<li>Panko</li>
<li>Rally</li>
<li>Sahara</li>
<li>Senlin</li>
<li>Solum</li>
<li>Swift</li>
<li>Tacker</li>
<li>Tempest</li>
<li>Trove</li>
<li>Vmtp</li>
<li>Watcher</li>
<li>Zaqar</li>
<li>Zun</li>
</ul>
<p><strong>支持的基础组件镜像有：</strong></p>
<ul>
<li><code>Ceph</code> implementation for Cinder, Glance and Nova</li>
<li><code>Collectd</code>, <code>InfluxDB</code>, and <code>Grafana</code> for performance monitoring.</li>
<li><code>Elasticsearch</code> and <code>Kibana</code> to search, analyze, and visualize log messages.</li>
<li><code>Etcd</code> a distributed key value store that provides a reliable way to store data across a cluster of machines.</li>
<li><code>Fluentd</code> as an open source data collector for unified logging layer.</li>
<li><code>HAProxy</code> and <code>Keepalived</code> for high availability of services and their endpoints.</li>
<li><code>Kafka</code> A distributed streaming platform.</li>
<li><code>MariaDB</code> and <code>Galera Cluster</code> for highly available MySQL databases.</li>
<li><code>Memcached</code> a distributed memory object caching system.</li>
<li><code>MongoDB</code> as a database back end for Ceilometer and Gnocchi.</li>
<li><code>Open vSwitch</code> and <code>Linuxbridge</code> back ends for Neutron.</li>
<li><code>RabbitMQ</code> as a messaging back end for communication between services.</li>
<li><code>Telegraf</code> as a plugin-driven server agent for collecting &amp; reporting metrics.</li>
</ul>
<h3 id="Kolla的架构"><a href="#Kolla的架构" class="headerlink" title="Kolla的架构"></a>Kolla的架构</h3><p>社区目前按照功能大概分成一下三个模块：</p>
<ul>
<li>Kolla，主要是负责Docker的镜像制作</li>
<li>Koola-Ansible 负责我能够取得配置管理</li>
<li>Kolla-Kubernetes 也是负责容器的配置管理</li>
</ul>
<p>kolla的Docker镜像制作，支持Radhat的rpm包，Ubuntu和Debian的Deb包，还能支持源码的方式。理论上源码制作的镜像，是可以跑在所有的支持容器的操作系统。</p>
<h3 id="Kolla解决的问题"><a href="#Kolla解决的问题" class="headerlink" title="Kolla解决的问题"></a>Kolla解决的问题</h3><p>采用Kolla来部署OpenStack，装好系统后，你大概只需要10分钟的时间，就可以搭建完成full feature的功能OpenStack。各种社区的最佳实践，高可用，都集成在上面。而且全都是运维人员都明白的python语言。</p>
<p>容器化后的OpenStack，让人感觉真的像积木一样，你需要什么，就拿过来放上去就可以。</p>
<p>也就是说，Kolla让以前很多OpenStack的部署，安装，升级的问题，解决起来更加优雅。</p>
<p>所谓升级就是把以前的删掉，再装新的版本。如果你是采用包的安装，例如rdo，那你就慢慢熬夜搞定吧，对于容器来说，做到这点就太简单了，非常优雅。</p>
<p>对于部署，已经没有安装的过程，你只需要把相应的容器放到相应的位置，配置管理推送过去就可以。对于升级，你只需要做一个容器的替换就可以实现升级，只需要集中精力去处理数据库的问题就可以。</p>
<p>build image的过程，其实可以官方提供，大家直接使用就可以。</p>
<blockquote>
<p>但目前测试，网络环境并不了乐观，如果直接拉取官网的镜像，基本会失败。</p>
</blockquote>
<p><strong>总结：</strong></p>
<ul>
<li>平滑的升级 / 回滚 OpenStack</li>
<li>保证环境的一致性 。 解决由于安装时间不同 ， 造成的包版本不一致的情况 。</li>
<li>支持多种安装源 ： 源代码安装 ，CentOS binary 安装等 。 可以替代掉 devstack。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;基本介绍&quot;&gt;&lt;a href=&quot;#基本介绍&quot; class=&quot;headerlink&quot; title=&quot;基本介绍&quot;&gt;&lt;/a&gt;基本介绍&lt;/h3&gt;&lt;p&gt;Kolla项目是2014年9月份，Steven Dake提交的，这位老兄以前是HeatPTL，还是Corosync作者，牛的一塌糊涂。对于OpenStack的项目是非常熟悉，并且以前是红帽工程师，目前跳槽到思科，代表思科推出Kolla项目。&lt;/p&gt;
    
    </summary>
    
      <category term="学习" scheme="https://www.hanbert.cn/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Openstack" scheme="https://www.hanbert.cn/tags/Openstack/"/>
    
      <category term="Kolla" scheme="https://www.hanbert.cn/tags/Kolla/"/>
    
      <category term="容器化" scheme="https://www.hanbert.cn/tags/%E5%AE%B9%E5%99%A8%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Devstack部署Openstack开发环境</title>
    <link href="https://www.hanbert.cn/2017/04/09/Devstack%E9%83%A8%E7%BD%B2Openstack%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/"/>
    <id>https://www.hanbert.cn/2017/04/09/Devstack部署Openstack开发环境/</id>
    <published>2017-04-09T12:01:19.000Z</published>
    <updated>2017-04-09T12:25:15.476Z</updated>
    
    <content type="html"><![CDATA[<p>Devstack 是面向 Openstack 开发者的快速自动化部署 Bash 脚本，提供了辅助开发和调试的源码环境，能够支持 All-In-One 和多节点部署模式，同时也支持 Plug-in 模式。Devstack 的使用可以说贯穿整个 Openstack 开发生涯，熟练的使用 Devstack 能有效提高开发效率。 </p>
<a id="more"></a>
<blockquote>
<p>正如官方所强调的：Devstack不适合生产环境！</p>
</blockquote>
<h3 id="支持的服务"><a href="#支持的服务" class="headerlink" title="支持的服务"></a>支持的服务</h3><p><strong>基础的操作系统</strong></p>
<p>OpenStack技术委员会把现在的CI策略定义为支持最新的Ubuntu发行版和最新的RHEL发行版（为了测试Python2.6）。</p>
<ul>
<li><strong>Ubuntu</strong>: current 长期支持版(LTS release)加上现在的开发版</li>
<li><strong>Fedora</strong>: 当前版本加上之前的版本</li>
<li><strong>RHEL</strong>: 当前的主要发行版本</li>
<li>其他OS平台可能继续支持，比如：<strong>Debian</strong>、<strong>OpenSUSE</strong>，但不是所有这些存在的平台都能得到支持</li>
<li>Ubuntu或是Fedora的补丁不会被支持，因为对其他的平台有副作用</li>
</ul>
<p><strong>数据库</strong></p>
<ul>
<li>MySQL</li>
<li>PostgreSQL</li>
</ul>
<p><strong>消息队列</strong></p>
<ul>
<li>Rabbit</li>
<li>Qpid</li>
</ul>
<p><strong>Web服务器</strong></p>
<ul>
<li>Apache</li>
</ul>
<p><strong>Openstack网络</strong></p>
<p>默认使用Nova Network，可以选定使用<code>Neutron</code></p>
<ul>
<li><strong>Nova Network</strong>: FlatDHCP</li>
<li><strong>Neutron</strong>: 使用<code>linuxbridge</code>或是 <code>OpenVSwitch</code>的基本配置，接近于初始的<code>FlatDHCP</code>方式。</li>
</ul>
<p><strong>Openstack服务</strong></p>
<p>DevStack中配置的默认服务有：</p>
<ul>
<li>认证（Keystone）</li>
<li>对象存储（Swift）</li>
<li>映像存储（Glance）</li>
<li>块存储（Cinder）</li>
<li>计算（Nova）</li>
<li>控制面板（Horizon）</li>
<li>编排（Heat）</li>
</ul>
<blockquote>
<p>其他没有直接包含在DevStack中的附加服务可以通过插件机制绑定在<code>stack.sh</code>里面<br>插件机制可以调用脚本来进行配置和启动相关服务</p>
</blockquote>
<p><strong>节点配置</strong></p>
<ul>
<li>单一节点</li>
<li>多节点，但不是重点支持对象，核心团队并不定期测试多节点的配置，即使测试也只包含最小配置</li>
</ul>
<h3 id="Devstack-的安装与使用"><a href="#Devstack-的安装与使用" class="headerlink" title="Devstack 的安装与使用"></a>Devstack 的安装与使用</h3><p>Devstack的安装过程比较简单，只需要从github库中git clone下来源码到本地，然后对本地做简单的配置即可。</p>
<p><strong>安装Linux</strong><br>本次使用的是Openstack官网推荐的<code>Ubuntu16.04</code>。<br>由于Devstack自动部署过程中很多地方都需要用到<code>git</code>的相关命令，所以我们先为操作系统安装<code>git</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># apt-get install git</div></pre></td></tr></table></figure></p>
<p><strong>添加<code>stack</code>用户</strong><br>Devstack必须运行在非<code>root</code>用户下，且该用户必须要<code>sudo</code>权限，本次为系统添加一个<code>stack</code>用户。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#切换到root用户</div><div class="line">$ su</div><div class="line">#添加账户，并指定shell和home所在地</div><div class="line"># useradd -s /bin/bash/ -d /opt/stack -m stack</div></pre></td></tr></table></figure></p>
<p>为刚添加的<code>stack</code>账户添加<code>sudo</code>权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># tee &lt;&lt;&lt; &quot;stack ALL=(ALL) NOPASSWD:ALL&quot; /etc/sudoers</div><div class="line"># su - stack</div></pre></td></tr></table></figure></p>
<blockquote>
<p>在Ubuntu中<code>sudoers</code>即使<code>root</code>用户默认也没有权限修改的，所以使用<code>tee</code>命令来写入，可以避免权限问题<br>也可以直接找到<code>/etc/shudoers</code>这个文件，改变其读写权限，然后打开文件写入，当不推荐这样做 </p>
</blockquote>
<p><strong>下载Devstack</strong></p>
<p>可以直接从github上下载最新版的额Devstack源码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ git clone https://git.openstack.org/openstack-dev/devstack</div><div class="line">$ cd devstack</div></pre></td></tr></table></figure></p>
<p>Download 下来的源码中就包含安装Openstack的脚本和相应的配置模板，理论上，现在就可以运行<code>stack.sh</code>脚本来安装Openstack。但往往为了方便我们开发使用，及保证安装的成功率，我们会在<code>local.conf</code>文件中进行一些配置，覆盖掉其默认的配置后，再进行安装。</p>
<p><strong>开始安装</strong></p>
<p>下载完Devstack，并做好配置后，就可以开始安装Openstack了，只需要运行<code>devstack/</code>目录下的<code>stack.sh</code>脚本既可以全自动进行安装，这期间会遇到一些问题，需要一点点手动解决。所以安装时间的长短因人而异。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./stack.sh</div></pre></td></tr></table></figure></p>
<p>由于默认Devstack是从github直接拉取源码进行安装，所以需要一个好的网络环境才可以，如果不具备，则可以考虑换一个靠谱点的<code>git</code>库，比如：<code>http://git.trystack.cn</code>，这个git库测试性能良好。如果一切顺利的话，大概20-30分钟可以完全部署完。如果遇到错误，需要手动解决ERROR后重新跑脚本，这样安装时间可能是具体情况无限制的延长。</p>
<h3 id="Devstack的配置过程"><a href="#Devstack的配置过程" class="headerlink" title="Devstack的配置过程"></a>Devstack的配置过程</h3><p>Devstack一致致力于通过最少的配置完成最多的功能。因为各个项目加入了新的特性，新的项目加入，还有不同的组合需要测试，选项的数目已经飞速地膨胀了。DevStack的传统做法是从<code>localrc</code>文件得到所有的本地化的配置和定制信息。传递给各个项目的配置变量的数目也在增加。原有的机制 (EXTRAS_OPTS之类)需要对每个文件进行编码，已经不太适应现在的环境。<br>在2013年10月引入了一个新的配置方式<code>local.conf</code>（参看<a href="https://review.openstack.org/#/c/46768/），希望能够简化配置过程，达到下列目的：" target="_blank" rel="external">https://review.openstack.org/#/c/46768/），希望能够简化配置过程，达到下列目的：</a></p>
<ul>
<li>在单一的文件中包含所有非默认的本地配置</li>
<li>与 localrc 的方式向后兼容，保证迁移过程的平滑</li>
<li>允许对任意配置文件中的设置进行更改</li>
</ul>
<blockquote>
<p>现在 Devstack 提供了两种配置安装的方式 local.conf(新版) 和 localrc(旧版),，两种方式我们都应该有所了解， 因为在不同的团队中会习惯的选择使用其中一种甚至两种方式。</p>
</blockquote>
<p><strong>local.conf</strong></p>
<p><code>local.conf</code>文件的样例路径为：<code>devstack/samples/local.conf</code></p>
<p>新的配置文件是<code>local.conf</code>，和旧的<code>localrc</code>文件在同一个目录。它是一个修正后的<code>INI</code>格式文件，引入了<code>meta-section</code>（<code>[[&lt;phase&gt;|&lt;config-file-name&gt;]]</code>）来承载额外的信息。改文件最终会被<code>stack.sh</code>脚本加载使用，所以其语法必须是符合<code>Bash</code>语法规则的，比如：等号“<code>=</code>”的两边不能存在空格。</p>
<p><code>Section</code>（也就是<code>&lt;phase&gt;</code>）有以下几种类型：”local”、”post-config”、”extra”、”post-extra”。它们规定了Devstack的安装流程和配置，在安装过程中会严格按照这个顺序进行读取和执行：</p>
<ol>
<li><strong>local</strong>：对应的Section为<code>[[local|localrc]]</code>，指定在<code>local.conf</code>被<code>stackrc</code>加载之前，先从<code>localrc</code>中提取配置项，配置实例如下：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[[<span class="built_in">local</span>|localrc]]</div><div class="line">ADMIN_PASSWORD=nomoresecret</div><div class="line">DATABASE_PASSWORD=stackdb</div><div class="line">RABBIT_PASSWORD=stackqueue</div><div class="line">SERVICE_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></div></pre></td></tr></table></figure>
<ol>
<li><strong>post-config</strong>：对应的Section为<code>[[post-config|/$Q_PLUGIN_CONF_FILE]]</code>，指定在项目服务自动配置完成后，且在服务正式启动之前，这其中的配置选项会被执行，配置实例如下：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[[post-config|/<span class="variable">$NOVA_CONF</span>]]</div><div class="line"></div><div class="line">[DEFAULT]</div><div class="line">use_syslog = True</div><div class="line"></div><div class="line">[osapi_v3]</div><div class="line">enabled = False</div><div class="line"></div><div class="line"><span class="comment"># <span class="doctag">NOTE:</span> Q_PLUGIN_CONF_FILE 独特之处在于它的配置项如果在前面不加 `/`， 那么这个配置项就不会生效。所以为了使其生效添加 `/` 是必须的。</span></div></pre></td></tr></table></figure>
<ol>
<li><strong>extra</strong>：指定在各Openstack项目的主服务启动之后，并且在<code>extra.d</code>中的文件别执行之前，<code>extra</code>的配置项会被执行</li>
<li><strong>post-extra</strong>：指定在<code>extra.d</code>中的文件被执行之后，执行<code>post-extra</code>中的配置项。</li>
</ol>
<blockquote>
<p>从以上可以看出，这么多的Section只是指定了其中的配置项在安装过程中的那个阶段被加载，一般情况下，我们只需要关注<code>local</code>这个Section即可，其他的稍作了解一下即可。</p>
</blockquote>
<p><code>[[local|localrc]]</code>是一个非常特别的Section，我们可以将配置项全部都定义到其下，并且它还指定了是否会将Devstack根目录下的<code>localrc</code>文件的配置项提取到其下，同时也允许所有的自定义安装配置项都包含在<code>localrc</code>文件中。</p>
<blockquote>
<p>这样做只是为了将Devstack的配置方式从<code>localrc</code>平滑过渡到<code>local.conf</code>，也就是说，可以将所有的安装配置项都定义到<code>localrc</code>，而无需修改<code>local.conf</code>文件。</p>
</blockquote>
<p><strong>一个最小化安装Openstack的<code>local.conf</code>配置样例</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[[<span class="built_in">local</span>|localrc]]</div><div class="line">HOST_IP=192.168.0.2 </div><div class="line">ADMIN_PASSWORD=&lt;YOUR_PASSWORD&gt;</div><div class="line">DATABASE_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></div><div class="line">RABBIT_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></div><div class="line">SERVICE_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></div><div class="line"><span class="comment">#FIXED_RANGE=172.31.1.0/24</span></div><div class="line"><span class="comment">#FLOATING_RANGE=192.168.20.0/25</span></div></pre></td></tr></table></figure>
<p>在上面的配置中，主要做了两件事情，第一、在<code>HOST_IP</code>字段指定本机的IP，这个是为了方面后面建立Endpoint，即所有服务的API用到的IP。理论上，Devstack可以自动检测本机IP，但是这个功能不能特别完善，有时候会出错，所有建议手动指定；第二、指定了各项需要服务的密码，<code>&lt;YOUR_PASSWORD&gt;</code>字段换成你自己想用的密码即可。<br><code>FIXED_RANGE</code>和<code>FLOATING_RANGE</code>两个字段可以用来指定内网和公网IP地址的范围，此处被注释掉了，使用其默认的网络配置。</p>
<h3 id="Devstack的部署流程及需要注意的问题"><a href="#Devstack的部署流程及需要注意的问题" class="headerlink" title="Devstack的部署流程及需要注意的问题"></a>Devstack的部署流程及需要注意的问题</h3><p>Devstack的自动化部署流程大概包含以下几个主要步骤：</p>
<ul>
<li>加载配置文件<code>local.conf</code>和<code>localrc</code>(如果用到了的话)</li>
<li>安装依赖包，此时是从系统的软件库中拉取的依赖包，包括安装<code>pip</code>的过程</li>
<li>安装消息队列和数据库</li>
<li>安装Openstack Clients</li>
<li>安装并配置Openstack的各项服务，此时为从github库中直接拉取源码</li>
<li>下载和上传镜像文件，从镜像的提供方官网下载的镜像</li>
<li>显示登录信息，出现这个信息说明部署成功了</li>
</ul>
<p><img src="http://img.blog.csdn.net/20170114134811369?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSm1pbGs=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="@自动化部署的流程"></p>
<p>其中下载依赖包，由于是从系统的软件库中直接边下载变安装的，所以建议提前对操作系统进行换源，以防以为网络问题导致安装失败。在<code>Centos7</code>和<code>Ubuntu16.04</code>两种操作系统上，经过测试阿里云源表现还是不错的：</p>
<ul>
<li><strong>Centos7系统更换阿里云源</strong>：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># cd /etc/yum.repos.d</span></div><div class="line"><span class="comment"># cp CentOS-Base.repo CentOS-Base.repo.bak //备份原来的源</span></div><div class="line"><span class="comment"># vim CentOS-Base.repo  //打开源的文件，删除原有的源，写入新的源</span></div></pre></td></tr></table></figure>
<p>清空原本<code>CentOS-Base.repo</code>的内容，写入如下的内容：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># CentOS-Base.repo</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># The mirror system uses the connecting IP address of the client and the</span></div><div class="line"><span class="comment"># update status of each mirror to pick mirrors that are updated to and</span></div><div class="line"><span class="comment"># geographically close to the client.  You should use this for CentOS updates</span></div><div class="line"><span class="comment"># unless you are manually picking other mirrors.</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># If the mirrorlist= does not work for you, as a fall back you can try the </span></div><div class="line"><span class="comment"># remarked out baseurl= line instead.</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment">#</span></div><div class="line"></div><div class="line">[base]</div><div class="line">name=CentOS-<span class="variable">$releasever</span> - Base - mirrors.aliyun.com</div><div class="line">failovermethod=priority</div><div class="line">baseurl=http://mirrors.aliyun.com/centos/<span class="variable">$releasever</span>/os/<span class="variable">$basearch</span>/</div><div class="line">        http://mirrors.aliyuncs.com/centos/<span class="variable">$releasever</span>/os/<span class="variable">$basearch</span>/</div><div class="line"><span class="comment">#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=os</span></div><div class="line">gpgcheck=1</div><div class="line">gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</div><div class="line"></div><div class="line"><span class="comment">#released updates </span></div><div class="line">[updates]</div><div class="line">name=CentOS-<span class="variable">$releasever</span> - Updates - mirrors.aliyun.com</div><div class="line">failovermethod=priority</div><div class="line">baseurl=http://mirrors.aliyun.com/centos/<span class="variable">$releasever</span>/updates/<span class="variable">$basearch</span>/</div><div class="line">        http://mirrors.aliyuncs.com/centos/<span class="variable">$releasever</span>/updates/<span class="variable">$basearch</span>/</div><div class="line"><span class="comment">#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updates</span></div><div class="line">gpgcheck=1</div><div class="line">gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</div><div class="line">[updates]</div><div class="line">name=CentOS-<span class="variable">$releasever</span> - Updates - mirrors.aliyun.com</div><div class="line">failovermethod=priority</div><div class="line">baseurl=http://mirrors.aliyun.com/centos/<span class="variable">$releasever</span>/updates/<span class="variable">$basearch</span>/</div><div class="line">        http://mirrors.aliyuncs.com/centos/<span class="variable">$releasever</span>/updates/<span class="variable">$basearch</span>/</div><div class="line"><span class="comment">#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updates</span></div><div class="line">gpgcheck=1</div><div class="line">gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</div><div class="line"></div><div class="line"><span class="comment">#additional packages that may be useful</span></div><div class="line">[extras]</div><div class="line">name=CentOS-<span class="variable">$releasever</span> - Extras - mirrors.aliyun.com</div><div class="line">failovermethod=priority</div><div class="line">baseurl=http://mirrors.aliyun.com/centos/<span class="variable">$releasever</span>/extras/<span class="variable">$basearch</span>/</div><div class="line">        http://mirrors.aliyuncs.com/centos/<span class="variable">$releasever</span>/extras/<span class="variable">$basearch</span>/</div><div class="line"><span class="comment">#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extras</span></div><div class="line">gpgcheck=1</div><div class="line">gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</div><div class="line"></div><div class="line"><span class="comment">#additional packages that extend functionality of existing packages</span></div><div class="line">[centosplus]</div><div class="line">name=CentOS-<span class="variable">$releasever</span> - Plus - mirrors.aliyun.com</div><div class="line">failovermethod=priority</div><div class="line">baseurl=http://mirrors.aliyun.com/centos/<span class="variable">$releasever</span>/centosplus/<span class="variable">$basearch</span>/</div><div class="line">        http://mirrors.aliyuncs.com/centos/<span class="variable">$releasever</span>/centosplus/<span class="variable">$basearch</span>/</div><div class="line"><span class="comment">#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplus</span></div><div class="line">gpgcheck=1</div><div class="line">enabled=0</div><div class="line">gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</div><div class="line"></div><div class="line"><span class="comment">#contrib - packages by Centos Users</span></div><div class="line">[contrib]</div><div class="line">name=CentOS-<span class="variable">$releasever</span> - Contrib - mirrors.aliyun.com</div><div class="line">failovermethod=priority</div><div class="line">baseurl=http://mirrors.aliyun.com/centos/<span class="variable">$releasever</span>/contrib/<span class="variable">$basearch</span>/</div><div class="line">        http://mirrors.aliyuncs.com/centos/<span class="variable">$releasever</span>/contrib/<span class="variable">$basearch</span>/</div><div class="line"><span class="comment">#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=contrib</span></div><div class="line">gpgcheck=1</div><div class="line">enabled=0</div><div class="line">gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</div></pre></td></tr></table></figure>
<p>然后执行以下命令，确定阿里云源已经正常工作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># yum update //升级本系统的软件</div></pre></td></tr></table></figure>
<ul>
<li><strong><code>Ubuntu16.04</code>更换阿里云源</strong>：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># cd /etc/apt/</span></div><div class="line"><span class="comment"># cp sources.list sources.list.bak //备份原有的源</span></div><div class="line"><span class="comment"># vim sources.list //打开源文件，清空原有的源，写入新的源</span></div></pre></td></tr></table></figure>
<p>清空<code>sources.list</code>中的原有内容，写入如下的内容：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricted</span></div><div class="line">deb-src http://archive.ubuntu.com/ubuntu xenial main restricted <span class="comment">#Added by software-properties</span></div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted</div><div class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe <span class="comment">#Added by software-properties</span></div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted</div><div class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe <span class="comment">#Added by software-properties</span></div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse</div><div class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse <span class="comment">#Added by software-properties</span></div><div class="line">deb http://archive.canonical.com/ubuntu xenial partner</div><div class="line">deb-src http://archive.canonical.com/ubuntu xenial partner</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted</div><div class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe <span class="comment">#Added by software-properties</span></div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse</div></pre></td></tr></table></figure>
<p>然后执行以下命令，确定阿里云源已经换源成功：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># apt-get update</span></div><div class="line"><span class="comment"># apt-get upgrade //升级本系统的软件</span></div></pre></td></tr></table></figure>
<blockquote>
<p>不同版本的操作系统，对应的源的地址不同，此处只写出了<code>centos7</code>和<code>ubuntu16.04</code>的操作系统对应的阿里云源，其他版本系统的请自行网上搜索。</p>
</blockquote>
<p><strong>安装pip过程中可能遇到的问题</strong></p>
<p>在安装<code>pip</code>的过程中，Devstack会从网上先下载一个<code>get-pip.py</code>的文件到<code>devstack/files/</code>文件夹下面，然后执行<code>python get-pip.py</code>运行这段程序来安装<code>pip</code>。所以当这个过程因为网络问题，导致<code>get-pip.py</code>不能够正常下载，导致安装失败的时候，我们可以手动下载<code>get-pip.py</code>文件，然后将其放入<code>devstack/files/</code>文件夹下面。</p>
<blockquote>
<p>除了上面的方法之外，我们也可以手动安装<code>pip</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; $ apt-get install python-pip  //安装pip</div><div class="line">&gt; $ pip install --upgrade pip  //升级到最新版本</div><div class="line">&gt;</div></pre></td></tr></table></figure></p>
</blockquote>
<p><strong>pip换源的问题</strong></p>
<p>由于Devstack部署过程中会用<code>pip</code>安装许多的软件包，而默认的<code>pip</code>的源的地址为<code>https://pypi.python.org/simple</code>这个地址在国内网络环境下访问不是很顺畅，时常出现下载包失败的情况，所以建议更换为国内源，此处换成了豆瓣源：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sodu mkdir /root/.pip/  //新建一个.pip文件夹</div><div class="line">$ vim /root/.pip/pip.conf //在新建的文件夹下面新建pip.conf文件</div></pre></td></tr></table></figure>
<p>然后在<code>pip.conf</code>文件中写入以下内容：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[global]</div><div class="line">index-url = https://pypi.douban.com/simple</div></pre></td></tr></table></figure>
<p>此时即可换源成功。</p>
<p><strong>pip安装软件包的问题</strong></p>
<p>如果换源后，<code>pip</code>中的某些软件包，仍然会从官方源下载，并且导致下载失败，可以手动停掉当然运行<code>stack.sh</code>脚本，把出错的安装命令复制一遍，手动指定安装源来进行安装，一般能够成功。命令格式为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ pip install &lt;YOUR_PACKAGES&gt; -i https://pypi.doubanio.com/simple</div></pre></td></tr></table></figure>
<p>这样该软件包就可以从我们指定的源地址进行下载安装。</p>
<blockquote>
<p>再部署过程中很多软件包<code>pip</code>自动安装失败，导致部署过程出错的问题，都可以通过手动指定软件源，手动安装，然后重新跑<code>stack.sh</code>来解决。</p>
</blockquote>
<p>另外，如果安装过程中出现如下错误：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">2016-04-10 08:40:55.596 | [ERROR] /home/devstack/<span class="built_in">functions</span>-common:1066 Failed to update apt repos, we’re dead now</div><div class="line">2016-04-10 08:40:56.600 | Error on <span class="built_in">exit</span></div><div class="line">2016-04-10 08:40:56.601 | ./stack.sh: line 488: generate-subunit: <span class="built_in">command</span> not found</div></pre></td></tr></table></figure>
<p>可以通过运行如下三条命令来解决：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install python-pip</div><div class="line">sudo pip install --upgrade pip</div><div class="line">sudo pip install -U os-testr -i https://pypi.douban.com/simple</div></pre></td></tr></table></figure>
<p><br><br><strong>长时间卡在运行<code>setup.py</code>程序</strong></p>
<p>如果运行过程中，出现长时间卡在这一步的情况，基本可以手动<code>Ctrl-C</code>停掉部署过程，然后向上翻输出的部署过程，找到如下内容：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo -H http_proxy= https_proxy= no_proxy= PIP_FIND_LINKS= SETUPTOOLS_SYS_PATH_TECHNIQUE=rewrite /usr/<span class="built_in">local</span>/bin/pip2.7 install -c /opt/stack/requirements/upper-constraints.txt <span class="_">-e</span> /opt/stack/keystone</div></pre></td></tr></table></figure>
<p>复制这段文字，然后手动运行这段命令，运行过程中如果出现安装源的问题，就用上面的<code>-i https://pypi.douban.com/simple</code>来制定安装源，如果能正常下载安装，就让它继续安装就可以，另外，此过程中如果出现某一个软件包安装时间过程也可以按一下<code>Crtl-C</code>，会自动重新安装这个软件包。</p>
<p><strong>Python虚拟环境搭建与使用</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ pip install virtualenv <span class="comment">#安装virtualenv工具</span></div><div class="line">$ virtualenv venvName <span class="comment"># 创建名为venvName的虚拟环境</span></div><div class="line">$ <span class="built_in">source</span> venvName/bin/activate <span class="comment"># 进入虚拟环境</span></div></pre></td></tr></table></figure>
<p>在部署过程中，Devstack会搭建好几个虚拟环境，这些虚拟环境都可以用上面的第三条命令来手动进入。</p>
<h3 id="Devstack-多节点部署"><a href="#Devstack-多节点部署" class="headerlink" title="Devstack 多节点部署"></a>Devstack 多节点部署</h3><p>Devstack大部分时候只是用来搭建一个<code>all in one</code>的开发测试环境，并不能够用于生产环境，但是作为一个部署工具，其本身也能够承担起多节点部署的任务。Devstack的多节点部署的本质就是：在不同的节点上，使用不同的<code>local.conf</code>配置文件来运行Devstack的部署脚本。但需要注意的是，Openstack的多节点部署不仅仅意味着不同的项目部署到不同的节点上，我们应该理解为，将Openstack不同的服务部署到不同的节点上，不同节点承担不同的功能。</p>
<p>一般来说，很少有人用Devstack来进行多节点的部署，这里可以只做了解即可。</p>
<p><code>Controller</code>节点的配置文件内容：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Misc</span></div><div class="line">ADMIN_PASSWORD=&lt;YOUR_PASSWORD&gt;</div><div class="line">DATABASE_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></div><div class="line">RABBIT_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></div><div class="line">SERVICE_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></div><div class="line">SERVICE_TOKEN=<span class="variable">$ADMIN_PASSWORD</span></div><div class="line"></div><div class="line"><span class="comment"># Target Path</span></div><div class="line">DEST=/opt/stack.mitaka</div><div class="line"></div><div class="line"><span class="comment"># Enable Logging</span></div><div class="line">LOGFILE=<span class="variable">$DEST</span>/logs/stack.sh.log</div><div class="line">VERBOSE=True</div><div class="line">LOG_COLOR=True</div><div class="line">SCREEN_LOGDIR=<span class="variable">$DEST</span>/logs</div><div class="line"></div><div class="line"><span class="comment"># Current host ip</span></div><div class="line">HOST_IP=192.168.56.102</div><div class="line">FLAT_INTERFACE=eth1</div><div class="line"></div><div class="line"><span class="comment"># 这个地方选择 True， 开启多节点部署</span></div><div class="line">MULTI_HOST=True</div><div class="line"></div><div class="line"><span class="comment"># 将应该部署到这个节点上的都 enable，部署到其他节点的都 disable</span></div><div class="line"><span class="comment"># Enable/Disable Nova/Cinder ControllerNode service</span></div><div class="line">enable_service n-novnc n-cauth</div><div class="line">disable_service n-cpu n-net n-api-meta c-vol</div></pre></td></tr></table></figure>
<p><code>computer</code>节点的配置文件内容：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Misc</span></div><div class="line">ADMIN_PASSWORD=&lt;YOUR_PASSWORD&gt;</div><div class="line">DATABASE_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></div><div class="line">RABBIT_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></div><div class="line">SERVICE_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></div><div class="line">SERVICE_TOKEN=<span class="variable">$ADMIN_PASSWORD</span></div><div class="line"></div><div class="line"><span class="comment"># Target Path</span></div><div class="line">DEST=/opt/stack.mitaka</div><div class="line"></div><div class="line"><span class="comment"># Enable Logging</span></div><div class="line">LOGFILE=<span class="variable">$DEST</span>/logs/stack.sh.log</div><div class="line">VERBOSE=True</div><div class="line">LOG_COLOR=True</div><div class="line">SCREEN_LOGDIR=<span class="variable">$DEST</span>/logs</div><div class="line"></div><div class="line"><span class="comment"># Current host ip</span></div><div class="line">HOST_IP=192.168.56.103</div><div class="line">FLAT_INTERFACE=eth1</div><div class="line"></div><div class="line"><span class="comment"># Enable Nova/Cinder ComputeNode service</span></div><div class="line">enable_service n-novnc n-cauth</div><div class="line">ENABLED_SERVICES=n-cpu,n-net,n-api-meta,c-vol</div><div class="line"></div><div class="line"><span class="comment"># Needed by cinder-volume service</span></div><div class="line">DATABASE_TYPE=mysql</div><div class="line"></div><div class="line"><span class="comment"># ControllerNode ipaddress</span></div><div class="line">SERVICE_HOST=192.168.56.102</div><div class="line">MYSQL_HOST=<span class="variable">$SERVICE_HOST</span></div><div class="line">RABBIT_HOST=<span class="variable">$SERVICE_HOST</span></div><div class="line">GLANCE_HOSTPORT=<span class="variable">$SERVICE_HOST</span>:9292</div><div class="line">NOVA_VNC_ENABLED=True</div><div class="line">NOVNCPROXY_URL=<span class="string">"http://<span class="variable">$SERVICE_HOST</span>:6080/vnc_auto.html"</span></div><div class="line">VNCSERVER_LISTEN=<span class="variable">$HOST_IP</span></div><div class="line">VNCSERVER_PROXYCLIENT_ADDRESS=<span class="variable">$VNCSERVER_LISTEN</span></div><div class="line"></div><div class="line">```bash</div><div class="line"></div><div class="line"><span class="comment">### 附：本次部署过程中用到的`local.conf`文件</span></div><div class="line"></div><div class="line">本次部署过程中，我们开启了`Sahara`服务，`ceilometer`服务，`aodh`服务和`heat`服务，这几项服务Devstack部署过程中默认是不安装的，需要手动开启，另外还有一些其他的组件也可以在部署过程中开启。具体的方法，可以在Openstack官网相应组件的介绍的`xxx and devstack`页面下找到。</div><div class="line"></div><div class="line">`local.conf`文件内容：</div><div class="line"></div><div class="line">```bash</div><div class="line">[[<span class="built_in">local</span>|localrc]]</div><div class="line"><span class="comment">############################################################</span></div><div class="line"><span class="comment"># Customize the following HOST_IP based on your installation</span></div><div class="line"><span class="comment">############################################################</span></div><div class="line">HOST_IP=10.10.87.5</div><div class="line"><span class="comment">#NEUTRON_CREATE_INITIAL_NETWORKS=False</span></div><div class="line">GIT_BASE=http://git.trystack.cn</div><div class="line">NOVNC_REPO=http://git.trystack.cn/kanaka/noVNC.git</div><div class="line">SPICE_REPO=http://git.trystack.cn/git/spice/spice-html5.git</div><div class="line">SERVICE_PASSWORD=123</div><div class="line">ADMIN_PASSWORD=123</div><div class="line">SERVICE_TOKEN=123</div><div class="line">IP_VERSION=4</div><div class="line">DATABASE_PASSWORD=123</div><div class="line">RABBIT_PASSWORD=123</div><div class="line"></div><div class="line"><span class="comment">############################################################</span></div><div class="line"><span class="comment"># Customize the following section based on your installation</span></div><div class="line"><span class="comment">############################################################</span></div><div class="line"></div><div class="line"><span class="comment"># Pip</span></div><div class="line"><span class="comment">#PIP_USE_MIRRORS=False</span></div><div class="line"><span class="comment">#USE_GET_PIP=1</span></div><div class="line"></div><div class="line"><span class="comment">#OFFLINE=False</span></div><div class="line"><span class="comment">#RECLONE=True</span></div><div class="line"><span class="comment">#ENABLE_IDENTITY_V2=False</span></div><div class="line"></div><div class="line"><span class="comment"># Logging</span></div><div class="line">LOGFILE=<span class="variable">$DEST</span>/logs/stack.sh.log</div><div class="line">SCREEN_LOGDIR=<span class="variable">$DEST</span>/logs/screen</div><div class="line">VERBOSE=True</div><div class="line">ENABLE_DEBUG_LOG_LEVEL=True</div><div class="line">ENABLE_VERBOSE_LOG_LEVEL=True</div><div class="line"></div><div class="line"><span class="comment"># Neutron ML2 with OpenVSwitch</span></div><div class="line"></div><div class="line"><span class="comment">#Q_PLUGIN=ml2</span></div><div class="line"><span class="comment">#Q_AGENT=openvswitch</span></div><div class="line"></div><div class="line"><span class="comment">#Q_DVR_MODE=dvr_snat</span></div><div class="line"></div><div class="line"><span class="comment">#enable_plugin gbp http://git.trystack.cn/openstack/group-based-policy master</span></div><div class="line"><span class="comment">#enable_service neutron</span></div><div class="line"><span class="comment">#enable_service q-svc</span></div><div class="line"><span class="comment">#enable_service q-agt</span></div><div class="line"><span class="comment">#enable_service q-meta</span></div><div class="line"><span class="comment">#enable_service q-dhcp</span></div><div class="line"><span class="comment">#enable_service q-l3</span></div><div class="line"><span class="comment">#disable_service n-net</span></div><div class="line"><span class="comment">#disable_service c-api cinder c-bak c-vol c-sch</span></div><div class="line"><span class="comment">#disable_service tempest</span></div><div class="line"><span class="comment">#disable_service heat h-api h-eng</span></div><div class="line"></div><div class="line"><span class="comment"># Swift</span></div><div class="line"><span class="comment"># -----</span></div><div class="line"></div><div class="line"><span class="comment"># Swift is now used as the back-end for the S3-like object store. Setting the</span></div><div class="line"><span class="comment"># hash value is required and you will be prompted for it if Swift is enabled</span></div><div class="line"><span class="comment"># so just set it to something already:</span></div><div class="line">SWIFT_HASH=66a3d6b56c1f479c8b4e70ab5c2000f5</div><div class="line"></div><div class="line"><span class="comment"># For development purposes the default of 3 replicas is usually not required.</span></div><div class="line"><span class="comment"># Set this to 1 to save some resources:</span></div><div class="line">SWIFT_REPLICAS=1</div><div class="line"></div><div class="line"><span class="comment"># The data for Swift is stored by default in (``$DEST/data/swift``),</span></div><div class="line"><span class="comment"># or (``$DATA_DIR/swift``) if ``DATA_DIR`` has been set, and can be</span></div><div class="line"><span class="comment"># moved by setting ``SWIFT_DATA_DIR``. The directory will be created</span></div><div class="line"><span class="comment"># if it does not exist.</span></div><div class="line">SWIFT_DATA_DIR=<span class="variable">$DEST</span>/data</div><div class="line"></div><div class="line"><span class="comment"># Enable sahara</span></div><div class="line">enable_plugin sahara http://git.trystack.cn/openstack/sahara</div><div class="line"></div><div class="line"><span class="comment"># Enable sahara-dashboard</span></div><div class="line">enable_plugin sahara-dashboard http://git.trystack.cn/openstack/sahara-dashboard</div><div class="line"></div><div class="line"><span class="comment">#Enable heat services</span></div><div class="line">enable_service h-eng h-api h-api-cfn h-api-cw</div><div class="line"></div><div class="line"><span class="comment">#Enable heat plugin</span></div><div class="line">enable_plugin heat http://git.trystack.cn/openstack/heat</div><div class="line"></div><div class="line">IMAGE_URL_SITE=<span class="string">"http://download.fedoraproject.org"</span></div><div class="line">IMAGE_URL_PATH=<span class="string">"/pub/fedora/linux/releases/25/CloudImages/x86_64/images/"</span></div><div class="line">IMAGE_URL_FILE=<span class="string">"Fedora-Cloud-Base-25-1.3.x86_64.qcow2"</span></div><div class="line">IMAGE_URLS+=<span class="string">","</span><span class="variable">$IMAGE_URL_SITE</span><span class="variable">$IMAGE_URL_PATH</span><span class="variable">$IMAGE_URL_FILE</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#Enable ceilometer and aodh service</span></div><div class="line">CEILOMETER_BACKEND=mongodb</div><div class="line">enable_plugin ceilometer http://git.trystack.cn/openstack/ceilometer</div><div class="line">enable_plugin aodh http://git.trystack.cn/openstack/aodh</div><div class="line"></div><div class="line"><span class="comment">#Enable OSprofiler service</span></div><div class="line">CEILOMETER_NOTIFICATION_TOPICS=notifications,profiler</div></pre></td></tr></table></figure>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://docs.openstack.org/developer/devstack/" target="_blank" rel="external">Devstack官网</a></li>
<li><a href="https://docs.openstack.org/developer/devstack/configuration.html" target="_blank" rel="external">Devstack配置文件详解</a></li>
<li><a href="http://lib.csdn.net/article/openstack/58949" target="_blank" rel="external">Openstack 实现技术分解 (1) 开发环境 — Devstack 部署案例详解</a></li>
<li><a href="http://blog.csdn.net/xjtuse_mal/article/details/7669371" target="_blank" rel="external"> Openstack 开发人员安装脚本解读</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_7643a1bf0102vhga.html" target="_blank" rel="external">DevStack剖析 （二）DevStack配置过程简述</a></li>
<li><a href="http://www.chenshake.com/openstack-deployment-tool-summary/" target="_blank" rel="external">Openstack部署工具总结——陈沙克日志</a></li>
<li><a href="http://www.chenshake.com/devstack-installation-and-testing/" target="_blank" rel="external">devstack安装和测试——陈沙克日志</a></li>
<li>国内的Openstack的git库: <a href="http://git.trystack.cn/cgit" target="_blank" rel="external">http://git.trystack.cn/cgit</a></li>
</ul>
<blockquote>
<ul>
<li>以上只是个人使用Devstack的一个学习过程的总结，其中难免有疏漏或者不当的地方，敬请指出！</li>
<li>文中部分内容来源于网络，在此对于原作者表示感谢！</li>
</ul>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Devstack 是面向 Openstack 开发者的快速自动化部署 Bash 脚本，提供了辅助开发和调试的源码环境，能够支持 All-In-One 和多节点部署模式，同时也支持 Plug-in 模式。Devstack 的使用可以说贯穿整个 Openstack 开发生涯，熟练的使用 Devstack 能有效提高开发效率。 &lt;/p&gt;
    
    </summary>
    
      <category term="学习" scheme="https://www.hanbert.cn/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Devstack" scheme="https://www.hanbert.cn/tags/Devstack/"/>
    
      <category term="Openstack" scheme="https://www.hanbert.cn/tags/Openstack/"/>
    
      <category term="开发环境" scheme="https://www.hanbert.cn/tags/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/"/>
    
  </entry>
  
  <entry>
    <title>数据库和数据仓库的区别</title>
    <link href="https://www.hanbert.cn/2016/12/10/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8C%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>https://www.hanbert.cn/2016/12/10/数据库和数据仓库的区别/</id>
    <published>2016-12-10T01:08:40.000Z</published>
    <updated>2017-04-10T01:47:41.333Z</updated>
    
    <content type="html"><![CDATA[<p>首先，定义三个概念：数据库软件、数据库、数据仓库。</p>
<a id="more"></a>
<h3 id="一、数据库软件"><a href="#一、数据库软件" class="headerlink" title="一、数据库软件"></a>一、数据库软件</h3><p>数据库软件：是一种软件，可以看得见，可以操作。用来实现数据库逻辑功能。属于物理层。</p>
<h3 id="二、数据库"><a href="#二、数据库" class="headerlink" title="二、数据库"></a>二、数据库</h3><p>数据库：是一种逻辑概念，用来存放数据的仓库，主要是为了处理在线数据，通过数据库软件来实现。<br>数据库由很多表组成，表是二维的，一张表里可以有很多字段。字段一字排开，对应的数据就一行一行写入表中。数据库的美，在于能够用二维表现多维关系。<br>目前市面上流行的数据库都是二维数据库。如：Oracle、DB2、MySQL、Sybase、MS SQL Server等。</p>
<h4 id="三、数据仓库"><a href="#三、数据仓库" class="headerlink" title="三、数据仓库"></a>三、数据仓库</h4><p>数据仓库：主要是为了处理历史数据。从逻辑上理解，数据库和数据仓库没有区别，都是通过数据库软件实现的存放数据的地方，只不过从数据量来说，数据仓库要比数据库更庞大得多。数据仓库主要用于数据挖掘和数据分析。</p>
<h4 id="四、数据库与数据仓库的区别"><a href="#四、数据库与数据仓库的区别" class="headerlink" title="四、数据库与数据仓库的区别"></a>四、数据库与数据仓库的区别</h4><p>在IT的架构体系中，数据库是必须存在的。必须要有地方存放数据。比如现在的网购，淘宝，京东等等。物品的存货数量，货品的价格，用户的账户余额之类的。这些数据都是存放在后台数据库中。</p>
<p>或者最简单理解，我们现在微博，QQ等账户的用户名和密码。在后台数据库必然有一张user表，字段起码有两个，即用户名和密码，然后我们的数据就一行一行的存在表上面。当我们登录的时候，我们填写了用户名和密码，这些数据就会被传回到后台去，去跟表上面的数据匹配，匹配成功了，你就能登录了。匹配不成功就会报错说密码错误或者没有此用户名等。这个就是数据库，数据库在生产环境就是用来干活的。凡是跟业务应用挂钩的，我们都使用数据库。</p>
<p>而数据仓库则是BI下的其中一种技术。由于数据库是跟业务应用挂钩的，所以一个数据库不可能装下一家公司的所有数据。数据库的表设计往往是针对某一个应用进行设计的。比如刚才那个登录的功能，这张user表上就只有这两个字段，没有别的字段了。</p>
<p>但是这张表符合应用，没有问题。但是这张表不符合分析。比如我想知道在哪个时间段，用户登录的量最多？哪个用户一年购物最多？诸如此类的指标。那就要重新设计数据库的表结构了。对于数据分析和数据挖掘，我们引入数据仓库概念。数据仓库的表结构是依照分析需求，分析维度，分析指标进行设计的。<br>数据仓库的数据来源于那些后台持续不停运作的数据库表。数据的搬运就牵涉到另一个技术叫ETL。这个过程就是数据从一个数据库到了数据仓库。</p>
<p><strong>举个例子：</strong></p>
<p>一家公司有5个分公司，月末要进行财务统计。那每家分公司都有自己的数据库可对自己分公司进行数据统计，可是，这5家分公司各自数据库的表结构设计都不同。可以理解为数据库中的表的数量不同，表中的字段也不同。如果要统计整个公司，那势必要制定统一标准。那就用到数据仓库，数据仓库作为一个新的汇总数据库，定义表的数量和字段内容。那各家分公司就要根据总公司的标准将自己数据库中的数据向总公司的字段安排靠拢。当然这个过程就交给ETL中的T来完成，transform转换。E是extract，L是load。抽取，导入。那这样，数据就被运送到数据仓库中了。统计学告诉我们，样本要足够多，得出的结论才能更准确，更具普遍性。这也就是要将各地的数据库数据汇总到一个数据仓库之中的原因。</p>
<h4 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h4><p>数据仓库所用的数据库软件，听的比较多的是Teradata。用下来感觉Teradata也是很强悍的。数据仓库对于数据软件的要求是非常高的，对硬件要求也高，所以能够用得起数据仓库的公司都是有钱的主。Teradata是软件与硬件绑定的。也就是说TD公司会给你送来一个大机柜，里面是一台计算机服务器和存储设备。服务器中已经安装好数据库软件了。数据挖掘，数据统计映射到数据库软件这里的操作，就是排序和分组。对于海量数据库来说，排序是很可怕的事情。数据仓库每天将承受住大量的外来数据往库里插入，往数据库里插数据，也是很慢的。所以数据仓库的技术在于调优，架构不解决根本性问题。而普通生产数据，可以通过架构来调整，相对来说更注重HA，一般做数据库集群。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;首先，定义三个概念：数据库软件、数据库、数据仓库。&lt;/p&gt;
    
    </summary>
    
      <category term="学习" scheme="https://www.hanbert.cn/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="数据库" scheme="https://www.hanbert.cn/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="数据仓库" scheme="https://www.hanbert.cn/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
      <category term="概念" scheme="https://www.hanbert.cn/tags/%E6%A6%82%E5%BF%B5/"/>
    
  </entry>
  
  <entry>
    <title>HBase 高可用（HA）集群部署</title>
    <link href="https://www.hanbert.cn/2016/10/11/HBase-%E9%AB%98%E5%8F%AF%E7%94%A8%EF%BC%88HA%EF%BC%89%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
    <id>https://www.hanbert.cn/2016/10/11/HBase-高可用（HA）集群部署/</id>
    <published>2016-10-11T01:12:33.000Z</published>
    <updated>2017-04-10T01:45:28.341Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍，在高可用（HA）Hadoop集群基础上搭建高可用（HA）的HBase集群。</p>
<a id="more"></a>
<h3 id="一、HBase-是什么"><a href="#一、HBase-是什么" class="headerlink" title="一、HBase 是什么"></a>一、HBase 是什么</h3><p>HBase是一个分布式的、面向列的开源数据库，就像Bigtable利用了Google文件系统（File System）所提供的分布式数据存储一样，HBase在Hadoop之上提供了类似于Bigtable的能力。HBase是Apache的Hadoop项目的子项目。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。另一个不同的是HBase基于列的而不是基于行的模式。它是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可在廉价PC Server上搭建起大规模结构化存储集群。</p>
<p><strong>HBase有如下使用场景：</strong></p>
<ul>
<li>大数据量 （100s TB级数据） 且有快速随机访问的需求。</li>
<li>例如淘宝的交易历史记录。数据量巨大无容置疑，面向普通用户的请求必然要即时响应。</li>
<li>容量的优雅扩展。</li>
<li>大数据的驱使，动态扩展系统容量的必须的。例如：webPage DB。</li>
<li>业务场景简单，不需要关系数据库中很多特性（例如交叉列、交叉表，事务，连接等等）。</li>
<li>优化方面：合理设计rowkey。因为hbase的查询用rowkey是最高效的，也几乎的唯一生产环境可行的方式。所以把你的查询请求转换为查询rowkey的请求吧。</li>
</ul>
<h3 id="二、HBase-该可用集群架构"><a href="#二、HBase-该可用集群架构" class="headerlink" title="二、HBase 该可用集群架构"></a>二、HBase 该可用集群架构</h3><p>由于我们是在已有的Hadoop可用集群基础上来搭建的，所以我们将<code>HMaster</code>分别部署在hadoop集群的<code>master1</code>和<code>master2</code>节点上，这样就能保证HBase的高可用性，放置单节点问题。将<code>RegionServer</code>部署在三个<code>slaver</code>节点上，分别是：<code>slaver1</code>、<code>slaver2</code>、<code>slaver3</code>。并且使用<code>zookeeper</code>做故障自动切换管理。</p>
<p><img src="http://img0.tuicool.com/FNJNvmm.png!web" alt="architecture"></p>
<h3 id="二、HBase-部署"><a href="#二、HBase-部署" class="headerlink" title="二、HBase 部署"></a>二、HBase 部署</h3><p>首先从官网下载：hbase-1.2.3-bin.tar.gz，下载链接为：<a href="http://apache.osuosl.org/hbase/stable/" target="_blank" rel="external">Click Me</a> 。然后上传到<code>master1</code>这个节点上。</p>
<h4 id="§-基础准备"><a href="#§-基础准备" class="headerlink" title="§ 基础准备"></a>§ 基础准备</h4><p>1、解压并重命名为<code>hbase</code>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># tar -axvf hbase-1.2.3-bin.tar.gz -C /opt</div><div class="line">// 重命名</div><div class="line"># mv hbase-1.2.3 hbase</div></pre></td></tr></table></figure></p>
<p>2、配置hbase的环境变量<code>/etc/profile</code>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">// 修改配置文件</div><div class="line"># sudo vi /etc/profile</div><div class="line"></div><div class="line">// 在最后下添加</div><div class="line">export HBASE_HOME=/opt/hbase</div><div class="line">export PATH=$PATH:$HBASE_HOME/bin</div><div class="line"></div><div class="line">// 刷新配置文件</div><div class="line"># source /etc/profile</div></pre></td></tr></table></figure></p>
<h4 id="§-修改hadoop配置文件，共3个"><a href="#§-修改hadoop配置文件，共3个" class="headerlink" title="§ 修改hadoop配置文件，共3个"></a>§ 修改hadoop配置文件，共3个</h4><p>1、HBase的所有配置文件都放在<code>/hbase/conf/</code>目录下，修改<code>hbase-env.sh</code>，将文件中的export JAVA_HOME=${JAVA_HOME}写成我们自己jdk的绝对路径：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"># The java implementation to use.  Java 1.7+ required.</div><div class="line">export JAVA_HOME=/opt/jdk8</div><div class="line"></div><div class="line"></div><div class="line"># Tell HBase whether it should manage it&apos;s own instance of Zookeeper or not.</div><div class="line">export HBASE_MANAGES_ZK=true</div></pre></td></tr></table></figure></p>
<p>2、修改<code>hbase-site.xml</code>文件，在<configuration></configuration>中间加入如下内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;!--这里注意了，这里的ns1用的是hadoop集群中hdfs的命名空间（namespace）！--&gt;</div><div class="line">    &lt;name&gt;hbase.rootdir&lt;/name&gt;</div><div class="line">    &lt;value&gt;hdfs://ns1/hbase&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;hbase.master&lt;/name&gt;</div><div class="line">    &lt;!--这里注意了，只需端口即可，不必再写主机名称了！--&gt;</div><div class="line">    &lt;value&gt;60000&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;!--可以不写端口号，只写主机名称，因为后面又配置端口号：2181--&gt;</div><div class="line">    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</div><div class="line">    &lt;value&gt;slaver1,slaver2,slaver3&lt;/value&gt;</div><div class="line">    &lt;description&gt;The directory shared by RegionServers.&lt;/description&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</div><div class="line">    &lt;value&gt;/opt/zookeeper/data&lt;/value&gt;</div><div class="line">    &lt;description&gt;Property from ZooKeeper config zoo.cfg. The directory where the snapshot is stored.&lt;/description&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;</div><div class="line">    &lt;value&gt;2181&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">       &lt;!--默认HMaster HTTP访问端口--&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;hbase.master.info.port&lt;/name&gt;</div><div class="line">    &lt;value&gt;16010&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">       &lt;!--默认HRegionServer HTTP访问端口--&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;hbase.regionserver.info.port&lt;/name&gt;</div><div class="line">    &lt;value&gt;16030&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<p>3、修改<code>regionservers</code>文件，加入所有需要加入HBase集群中的从节点：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">slaver1</div><div class="line">slaver2</div><div class="line">slaver3</div><div class="line">~                                                                                                                                                                                            </div><div class="line">~</div></pre></td></tr></table></figure></p>
<h4 id="§-将配置好的文件分发到其余各个节点"><a href="#§-将配置好的文件分发到其余各个节点" class="headerlink" title="§ 将配置好的文件分发到其余各个节点"></a>§ 将配置好的文件分发到其余各个节点</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># cd /opt</div><div class="line"># scp -r ./hbase master2:/opt</div><div class="line"># scp -r ./hbase slaver1:/opt</div><div class="line"># scp -r ./hbase slaver2:/opt</div><div class="line"># scp -r ./hbase slaver3:/opt</div></pre></td></tr></table></figure>
<h4 id="§-启动HBase集群"><a href="#§-启动HBase集群" class="headerlink" title="§ 启动HBase集群"></a>§ 启动HBase集群</h4><p>在启动HBase之前，必须先启动<code>zookeeper</code>集群和<code>hadoop</code>集群。<br>1、在<code>master1</code>上面，运行以下命令，如果没有配置好hbase的环境变量，可以到<code>hbase/bin</code>目录下执行 ./start-hbase.sh：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># start-hbase.sh</div></pre></td></tr></table></figure></p>
<p>2、在<code>master2</code>上，单独启动一个<code>HMaster</code>进程：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># hbase-daemon.sh start master</div></pre></td></tr></table></figure></p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>中间的<code>start</code>参数换成<code>stop</code>就成了停止指定进程的命令。</li>
</ul>
</blockquote>
<p>3、HBase集群停止命令，在<code>master1</code>上运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># stop-hbase.sh</div></pre></td></tr></table></figure></p>
<p>4、验证已经正常启动：<br>在各个节点运行<code>jps</code>命令可以看到一下进程，其中<code>HMaster</code>和<code>HRegionServer</code>为HBase的相关进程：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">// master1上运行的进程，前面的数字对应的进程的pid号</div><div class="line">3376 HMaster</div><div class="line">977 ResourceManager</div><div class="line">32756 DFSZKFailoverController</div><div class="line">3657 Jps</div><div class="line">522 NameNode</div><div class="line"></div><div class="line">// master2上运行的进程，前面的数字对应的进程的pid号</div><div class="line">30995 Jps</div><div class="line">28164 NameNode</div><div class="line">28502 ResourceManager</div><div class="line">30344 HMaster</div><div class="line">28059 DFSZKFailoverController</div><div class="line"></div><div class="line">// slaver1上运行的进程，前面的数字对应的进程的pid号</div><div class="line">13520 DataNode</div><div class="line">13367 JournalNode</div><div class="line">14488 HRegionServer</div><div class="line">13690 NodeManager</div><div class="line">14762 Jps</div><div class="line">4523 QuorumPeerMain</div><div class="line"></div><div class="line">// slaver2上运行的进程，前面的数字对应的进程的pid号</div><div class="line">21744 DataNode</div><div class="line">21591 JournalNode</div><div class="line">13450 QuorumPeerMain</div><div class="line">21915 NodeManager</div><div class="line">22972 Jps</div><div class="line">22686 HRegionServer</div><div class="line"></div><div class="line">// slaver3上运行的进程，前面的数字对应的进程的pid号</div><div class="line">22898 DataNode</div><div class="line">22744 JournalNode</div><div class="line">23848 HRegionServer</div><div class="line">24105 Jps</div><div class="line">17210 QuorumPeerMain</div><div class="line">23069 NodeManager</div></pre></td></tr></table></figure></p>
<p>5、通过hadoop的web页面，查看是否异常，以及高可用是否正常运行：</p>
<p>在浏览器中输入：<a href="http://master1:16010，其中如果本地hosts中没有建立相应的主机名与IP的映射关系，将master1换成相应的IP，显示如下：" target="_blank" rel="external">http://master1:16010，其中如果本地hosts中没有建立相应的主机名与IP的映射关系，将master1换成相应的IP，显示如下：</a></p>
<p><img src="http://img.blog.csdn.net/20161017234127072" alt="hbaseHA"></p>
<p>在浏览器中输入：<a href="http://master2:16010，其中如果本地hosts中没有建立相应的主机名与IP的映射关系，将master2换成相应的IP，显示如下：" target="_blank" rel="external">http://master2:16010，其中如果本地hosts中没有建立相应的主机名与IP的映射关系，将master2换成相应的IP，显示如下：</a></p>
<p><img src="http://img.blog.csdn.net/20161017234225172" alt="hbaseHA"></p>
<p><strong>由以上两张图可以看出，目前<code>master1</code>处于<code>active</code>状态，<code>master2</code>处于<code>standby</code>状态，说明HBase的HA已成功建立。</strong></p>
<h3 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h3><p>总的来说，HBase高可用集群的搭建还是相当的简单的。但有时候需要注意一下这个问题：如果HBase启动过程中显示不能够正常搜索HDFS的<code>namespace</code>或者不能解析HDFS路径，可以将<code>/opt/hadoop/etc/hadoop</code>下的<code>core-site.xml</code> 和<code>hdfs-site.xml</code>拷到<code>/opt/hbase/conf/</code>下，一般可以解决问题。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍，在高可用（HA）Hadoop集群基础上搭建高可用（HA）的HBase集群。&lt;/p&gt;
    
    </summary>
    
      <category term="学习" scheme="https://www.hanbert.cn/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Hadoop" scheme="https://www.hanbert.cn/tags/Hadoop/"/>
    
      <category term="高可用" scheme="https://www.hanbert.cn/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    
      <category term="HBase" scheme="https://www.hanbert.cn/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 2.7.3 高可用（HA）集群部署</title>
    <link href="https://www.hanbert.cn/2016/10/10/Hadoop-2-7-3-%E9%AB%98%E5%8F%AF%E7%94%A8%EF%BC%88HA%EF%BC%89%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
    <id>https://www.hanbert.cn/2016/10/10/Hadoop-2-7-3-高可用（HA）集群部署/</id>
    <published>2016-10-10T01:12:29.000Z</published>
    <updated>2017-04-10T01:45:05.941Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍如何搭建HDFS（NameNode）和ResourceManager高可用的Hadoop集群。</p>
<a id="more"></a>
<p><strong>HDFS的高可用（HA）的实现方式：</strong></p>
<p>  一种是将NN维护的元数据保存一份到NFS上，当NN故障，可以通过另一台NNe读取NFS目录中的元数据备份进行恢复工作，需要手动进行操作，并不是真正意义上的HA方案。</p>
<p>  另一种是准备一台备用NN节点，通过定期下载NN的元数据和日志文件来备份，当NN故障时，可以通过这台进行恢复，由于主备节点元数据和日志并不是实时同步，所以会丢失一些数据。</p>
<p>  前两种方案都不是很理想，社区提供一种更好的方案，基于QJM（Qurom Journal Manager）的共享日志方案。QJM的基本原理是NN（Active）把日志写本地和2N+1（奇数）台JournalNode上，当数据操作返回成功时才写入日志，这个日志叫做editlog，而元数据存在fsimage文件中，NN（Standby）定期从JournalNode上读取editlog到本地。在这手动切换的基础上又开发了基于Zookeeper的ZKFC（ZookeeperFailover Controller）自动切换机制，Active和Standby节点各有ZKFC进程监控NN监控状况，定期发送心跳，当Active节点故障时Standby会自动切换为ActiveNode，本次就用的此方案。如下图所示：</p>
<p><img src="http://s3.51cto.com/wyfs02/M02/6E/60/wKioL1V6meKwAMxFAAFOKORH5AM212.jpg" alt="HDFS-HA"></p>
<p><strong>ResourceManager(RM) HA实现方式：</strong></p>
<p>  RM将状态信息存储在Zookeeper中，当Active故障，Standby切换为Active后，从ZK读取相应的作业信息，重新构建作业的内存信息，然后开始接受NodeManager心跳，并接受客户端提交作业的请求等。</p>
<h3 id="一、部署前的准备工作"><a href="#一、部署前的准备工作" class="headerlink" title="一、部署前的准备工作"></a>一、部署前的准备工作</h3><ul>
<li>OpenStack 平台，构建虚拟机，也可用VMWare代替</li>
<li>Centos7 x64 操作系统</li>
<li>Hadoop 2.7.3 64位安装包（bin版）</li>
<li>Zookeeper 3.4.9 安装包（bin版）</li>
</ul>
<blockquote>
<p><strong>Apache 镜像下载地址：</strong><a href="http://www.apache.org/dist/" target="_blank" rel="external">http://www.apache.org/dist/</a><br><strong>Hadoop官方文档：</strong><a href="http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/ClusterSetup.html" target="_blank" rel="external">Hadoop Document</a></p>
</blockquote>
<h3 id="二、集群规划"><a href="#二、集群规划" class="headerlink" title="二、集群规划"></a>二、集群规划</h3><table>
<thead>
<tr>
<th style="text-align:left">主机名</th>
<th style="text-align:left">IP</th>
<th style="text-align:left">安装的软件</th>
<th style="text-align:left">运行的进程</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">master1</td>
<td style="text-align:left">172.16.1.33</td>
<td style="text-align:left">hadoop</td>
<td style="text-align:left">NameNode(active)、DFSZKFailoverController、ResourceManager(active)</td>
</tr>
<tr>
<td style="text-align:left">master2</td>
<td style="text-align:left">172.16.1.35</td>
<td style="text-align:left">hadoop</td>
<td style="text-align:left">NameNode(Standby)、DFSZKFailoverController、ResourceManager(Standby)</td>
</tr>
<tr>
<td style="text-align:left">slaver1</td>
<td style="text-align:left">172.16.1.22</td>
<td style="text-align:left">hadoop、zookeeper</td>
<td style="text-align:left">DataNode、NodeManager、QuorumPeerMain、JournalNode</td>
</tr>
<tr>
<td style="text-align:left">slaver2</td>
<td style="text-align:left">172.16.1.23</td>
<td style="text-align:left">hadoop、zookeeper</td>
<td style="text-align:left">DataNode、NodeManager、QuorumPeerMain、JournalNode</td>
</tr>
<tr>
<td style="text-align:left">slaver3</td>
<td style="text-align:left">172.16.1.24</td>
<td style="text-align:left">hadoop、zookeeper</td>
<td style="text-align:left">DataNode、NodeManager、QuorumPeerMain、JournalNode</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Note：</strong></p>
<ul>
<li>如果节点资源足够多，也可以吧ResourceManager单独安装在两个节点上，这样更符合HA的特性</li>
<li>除了必须有两个NameNode之外，DataNode的节点可以尽可能的多，配置方式一样</li>
</ul>
</blockquote>
<h3 id="三、基础环境配置"><a href="#三、基础环境配置" class="headerlink" title="三、基础环境配置"></a>三、基础环境配置</h3><h4 id="§-更改主机名，并建立主机名与IP的映射关系（每个节点都要做）"><a href="#§-更改主机名，并建立主机名与IP的映射关系（每个节点都要做）" class="headerlink" title="§ 更改主机名，并建立主机名与IP的映射关系（每个节点都要做）"></a>§ 更改主机名，并建立主机名与IP的映射关系（每个节点都要做）</h4><p>1、临时更改主机名，机器重启后失效，以<code>master1</code>为例，其余每个节点同样设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># hostname master1</div><div class="line"># hostname</div><div class="line">master1</div></pre></td></tr></table></figure>
<p>2、永久修改主机名<br>修改centos网络配置文件<code>/etc/sysconfig/network</code>，在末尾添加<code>HOSTNAME=master1</code>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># vim /etc/sysconfig/network</div><div class="line">NETWORKING=yes</div><div class="line">NOZEROCONF=yes</div><div class="line">HOSTNAME=master1</div><div class="line">~                                                                                                                                                                                            </div><div class="line">~</div></pre></td></tr></table></figure></p>
<p>3、修改<code>/etc/hosts</code>文件，最终状态，节点中的每台主机都要修改好相应的主机名，并在<code>hosts</code>文件中写入相应的IP 和主机名的映射关系，状态如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># vim /etc/hosts</div><div class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</div><div class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</div><div class="line">172.16.1.33 master1</div><div class="line">172.16.1.35 master2</div><div class="line">172.16.1.22 slaver1</div><div class="line">172.16.1.23 slaver2</div><div class="line">172.16.1.24 slaver3</div><div class="line">~    </div><div class="line">~</div></pre></td></tr></table></figure></p>
<p>这样是永久修改，重启后生效，也可以结合第一种方法，让该用户名立即生效。</p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>在<code>/etc/</code>下有一个<code>hostname</code>文件，如果是Ubuntu系统可以直接在这个文件下面写入想要设置的主机名来达到永久修改主机名。</li>
<li>经过验证，openstack生成的虚拟机中，<code>hostname</code>会写有一个类似于<code>test.novalocal</code>的以<code>*.novalocal</code>结尾的主机名，如果按照<strong>方法2</strong>修改主机名，每次重启后，会自动把主机名设成这个名字，而不能如愿更改为我们想要的主机名，即使在<code>hostname</code>文件中，删掉原有主机名，写上我们想要设置的主机名，重启后以后会还原为删掉之前的样子。</li>
<li>所以，在本次配置中，我们使用<strong>方法1</strong>来临时修改主机名，尽量不重启机器，如果需要重启机器，重启后重新修改主机名，以防Hadoop不能正常启动。</li>
</ul>
</blockquote>
<p>4、如果机器的时间差相差太大容易导致启动失败，所以提前进行时间同步：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># yum install ntp</div><div class="line"># ntpdate ntp.fudam.edu.cn</div></pre></td></tr></table></figure></p>
<h4 id="§-关闭防火墙（每个节点都要配置）"><a href="#§-关闭防火墙（每个节点都要配置）" class="headerlink" title="§ 关闭防火墙（每个节点都要配置）"></a>§ 关闭防火墙（每个节点都要配置）</h4><p>如果是生产环境中可以通过配置iptables规则来开放端口，此处我们作为实验且私网环境直接关闭放火墙来达到目的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">// 关闭防火墙</div><div class="line"># sudo systemctl stop firewalld.service </div><div class="line">// 关闭开机启动</div><div class="line"># sudo systemctl disable firewalld.service</div></pre></td></tr></table></figure></p>
<h4 id="§-创建专门的用户（每个节点都要配置）"><a href="#§-创建专门的用户（每个节点都要配置）" class="headerlink" title="§ 创建专门的用户（每个节点都要配置）"></a>§ 创建专门的用户（每个节点都要配置）</h4><p>在安装完Centos7后，如果在真实的生产环境中，最好建立一个新的用户和组，专门用来安装Hadoop。<br>1、在root下，创建组和用户，为每一台虚拟机创建一个Hadoop用户<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">// 先创建组cloud</div><div class="line"># groupadd cloud</div><div class="line">// 创建用户并加入组cloud</div><div class="line">useradd -g cloud hadoop</div><div class="line">// 修改用户hadoop的密码</div><div class="line"># passwd hadoop</div></pre></td></tr></table></figure></p>
<p>2、将hadoop用户加到soduers列表，获取root权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">// 查看/etc/sudoers的权限</div><div class="line"># ls -l /etc/sudoers</div><div class="line">// 修改权限为 777</div><div class="line"># chmod 777 /etc/sudoers</div><div class="line">// 将hadoop添加root权限</div><div class="line"># vim /etc/sudoers</div><div class="line">## The COMMANDS section may have other options added to it.</div><div class="line">##</div><div class="line">## Allow root to run any commands anywhere </div><div class="line">root    ALL=(ALL)       ALL</div><div class="line">hadoop   ALL=(ALL)       ALL</div><div class="line"></div><div class="line">// 还原权限</div><div class="line"># chmod 440 /etc/sudoers</div></pre></td></tr></table></figure></p>
<p>其他节点的机器同样操作。</p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>本次部署，我们直接用的root用户，而没有新建hadoop用户和cloud组</li>
<li>相比于一台一台的修改配置文件，可以先在一台上面修改，然后用<code>scp</code>命令分发到其余各个节点的相应位置</li>
</ul>
</blockquote>
<h4 id="§-配置ssh免密登录（每个节点都要配置）"><a href="#§-配置ssh免密登录（每个节点都要配置）" class="headerlink" title="§ 配置ssh免密登录（每个节点都要配置）"></a>§ 配置ssh免密登录（每个节点都要配置）</h4><p>hadoop在使用过程中需要分发好多文件，配置好免密登录可以免去我们要不断地输入密码的麻烦。也有助于我们部署过程中把自己修改的配置文件分发到各个节点。</p>
<p>1、生成公钥和私钥，过程中会有一些提示选项，直接按回车，采用默认值便好</p>
<pre><code># ssh-keygen -t rsa
</code></pre><p>2、执行完上面的命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥），将公钥拷贝到要免登陆的机器上：</p>
<pre><code># ssh-copy-id 172.16.1.33
# ssh-copy-id 172.16.1.35
# ssh-copy-id 172.16.1.22
# ssh-copy-id 172.16.1.23
# ssh-copy-id 172.16.1.24
</code></pre><p>3、这时会在相应的主机的<code>~/.ssh/</code>下产生一个名为<code>authorized_keys</code>的文件，这时通过 <code>ssh 172.16.1.35</code>(修改为相应的主机对应的IP) 时可以直接免密登陆进入主机。<br>4、其他的每一个节点同样操作。</p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>也可以选择先在每一个节点上生成相应的私钥和公钥，然后把公钥通过用<code>scp</code>命令发送到一台机器上，比如<code>master1</code>，然后统一一块加入到这台机器<code>~/.ssh/authorized_keys</code>文件里面</li>
<li>然后把这个文件用<code>scp</code>分发到每一台机器的<code>~/.ssh/</code>文件夹中</li>
<li>这样就实现了集群中每一个节点的相互免密登录，而且，省去了每个节点都要手动配置的麻烦</li>
</ul>
</blockquote>
<h3 id="四、安装JDK（每个节点都要配置）"><a href="#四、安装JDK（每个节点都要配置）" class="headerlink" title="四、安装JDK（每个节点都要配置）"></a>四、安装JDK（每个节点都要配置）</h3><p>本次是用的是 jdk-8u101-linux-x64.tar.gz ，可以从Oracle官网下载，然后传送到每一台机器上，并解压，解压的路径可以自由选择，本次选择<code>/opt/</code>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># tar -zxvf jdk-8u101-linux-x64.tar.gz -C /opt</div><div class="line">// 修改文件夹名字</div><div class="line"># # mv /opt/jdk1.8.0_101 /opt/jdk8</div></pre></td></tr></table></figure></p>
<p>配置环境变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">// 修改配置文件</div><div class="line"># sudo vim /etc/profile</div><div class="line">	</div><div class="line">// 在最后下添加</div><div class="line">export JAVA_HOME=/opt/jdk8</div><div class="line">export PATH=$JAVA_HOME/bin:$PATH</div><div class="line">export CLASSPATH=.:$JAVA_HOME/lib</div><div class="line">	</div><div class="line">// 刷新配置文件</div><div class="line"># source /etc/profile</div></pre></td></tr></table></figure>
<p>其他每台机器都做同样的配置，或者将这个配好的jdk和profile文件用<code>scp</code>命令分发到每一台机器上。</p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>jdk的安装目录尽量不要选在普通用户的<code>/home/USER_NAME</code>家目录下，因为在后面<code>hadoop</code>配置中需要用到这个jdk的目录的绝对路径，如果写到<code>/home/USER_NAME</code>这个里面，其中的<code>/USER_NAME</code>会根据分发到各节点的用户名不同而不同，所以要在<code>hadoop</code>中重新配置这个<code>JAVA_HOME</code>的绝对路径，否则会导致启动失败。</li>
</ul>
</blockquote>
<h3 id="五、安装zookeeper（所有的slaver节点都要安装）"><a href="#五、安装zookeeper（所有的slaver节点都要安装）" class="headerlink" title="五、安装zookeeper（所有的slaver节点都要安装）"></a>五、安装zookeeper（所有的slaver节点都要安装）</h3><p>在slaver1、slaver2、slaver3上安装zookeeper。<br>1、从Apache官网下载zookeeper-3.4.9.tar.gz，并上传到以上三台节点的任意一台上面，然后解压到<code>/opt</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># tar -zxvf zookeeper-3.4.8.tar.gz -C /opt</div><div class="line">// 重命名文件夹</div><div class="line"># mv zookeeper-3.4.8 zookeeper</div></pre></td></tr></table></figure></p>
<p>2、修改zookeeper的默认配置文件<code>./zookeeper/conf/zoo.cfg</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># mv zoo_sample.cfg zoo.cfg</div><div class="line"># vim zoo.cfg</div></pre></td></tr></table></figure></p>
<p>3、在打开的文件中修改或添加一下内容：</p>
<p> 修改dataDir指向我们数据文件夹，默认没有这个文件夹，需要后面的步骤创建:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dataDir=/opt/zookeeper/data</div><div class="line">dataLogDir=/opt/zookeeper/logs</div></pre></td></tr></table></figure></p>
<p>并在最后添加：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">server.1=slaver1:2888:3888</div><div class="line">server.2=slaver2:2888:3888</div><div class="line">server.3=slaver3:2888:3888</div></pre></td></tr></table></figure></p>
<blockquote>
<p><strong>NOTE:</strong></p>
<p>此文件中的部分参数说明：</p>
<ul>
<li>tickTime：ZK服务器之间或客户端与服务器之间间隔多长时间发送一个心跳，单位毫秒</li>
<li>initLimit：ZK服务器集群中连接Leader的Follower服务器初始化连接时最长忍受多长心跳时间间隔（5*20000=10s）</li>
<li>syncLimit：标识Leader与Follower同步消息，如果超过时间（5*2000=10s），未完成同步，将剔除这个节点，所有连接此&gt; &gt; - Follower服务器的客户端将连接到另一个Foolower服务器上</li>
<li>dataDir：ZK保存数据的目录，默认情况下，ZK也会将日志文件保存在此目录</li>
<li>dataLogDir：指定日志文件目录</li>
<li>clientPort：客户端连接ZK服务器端口</li>
<li>server.1：第一个1代表第几号ZK服务器，slaver1是这个服务器的主机名或IP，2888是这个ZK服务器与集群中Leader服务器交换信息的端口，3888是Leader服务器出现故障时，用这个端口通信重新选举，在选出一个新的Leader</li>
</ul>
</blockquote>
<p>4、创建相应的目录和<code>myid</code>文件，其中，这个<code>myid</code>文件必须创建，否则启动会报错。分别在ZK集群节点创建myid号，myid一定对应好zoo.cfg中配置的server后面1、2、3这个ZK号，即本次部署过程中，<code>myid</code>文件的内容在slaver1上为1、在slaver2上为2、在slaver3上为3：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># mkdir /opt/zookeeper/data</div><div class="line"># mkdir /opt/zookeeper/logs</div><div class="line"># vim /opt/zookeeper/data/myid  </div><div class="line">1</div></pre></td></tr></table></figure></p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>上面的这个<code>/opt/zookeeper/data</code>等文件夹也可以换到别的目录创建，不一定非得在代码所在目录下，只需要在上面那个<code>zoo.cfg</code>指定即可。比如<code>/home/zookeeper/data</code>。同理<code>logs</code>文件夹也是。</li>
</ul>
</blockquote>
<p>5、分别启动所有的ZK节点（在所有的slaver上操作）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/opt/zookeeper/bin/zkServer.sh start</div></pre></td></tr></table></figure></p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>ZK的每次启动都得单独一台一台的启动，不能够通过hadoop来统一管理启动！</li>
</ul>
</blockquote>
<p>6、检查启动是否成功，在三台slaver上运行下面的命令，就能找到唯一的一个<code>leader</code>和剩余的全是<code>follower</code>:</p>
<pre><code># /opt/zookeeper/bin/zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /opt/zookeeper/bin/../conf/zoo.cfg
Mode: follower
</code></pre><p>运行<code>jps</code>来查看相应的java进程，能够看到：</p>
<pre><code># jps
4523 QuorumPeerMain
11503 Jps
</code></pre><p>以上说明zookeeper已经成功安装并启动。</p>
<p>7、zookeeper停止命令：</p>
<pre><code>/opt/zookeeper/bin/zkServer.sh stop
</code></pre><p>需要在每一个需要停止zookeeper的节点上运行。</p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>如果我们就找到<code>leader</code>那台机器，然后运行上面的命令停掉ZK，或者<code>jps</code>查到<code>QuorumPeerMain</code>的<code>pid</code>号，然后用<code>kill</code>命令</li>
</ul>
</blockquote>
<p>8、设置zookeeper的环境变量，在<code>/etc/profile</code>文件末尾添加如下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># vim /etc/profile</div><div class="line"></div><div class="line">export ZOOKEEPER_HOME=/opt/zookeeper</div><div class="line">export PATH=$ZOOKEEPER_HOME/bin:$PATH</div><div class="line"></div><div class="line"># source /etc/profile</div></pre></td></tr></table></figure></p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>如果提前在<code>/etc/profile</code>文件中写入<code>export</code>的zookeeper的环境变量，便可以不用每次执行命令都先<code>cd</code>到相应的文件夹下，下面部署过程中配置环境变量也是同样的道理。</li>
</ul>
</blockquote>
<h3 id="六、安装hadoop（所有的节点都要安装）"><a href="#六、安装hadoop（所有的节点都要安装）" class="headerlink" title="六、安装hadoop（所有的节点都要安装）"></a>六、安装hadoop（所有的节点都要安装）</h3><p> 先从官网下载hadoop-2.7.3.tar.gz，上传到<code>master1</code>这台机器上。先在<code>master1</code>上解压，配置，然后用<code>scp</code>命令分发到其余各个节点即可。</p>
<h4 id="§-解压hadoop并配置环境变量"><a href="#§-解压hadoop并配置环境变量" class="headerlink" title="§ 解压hadoop并配置环境变量"></a>§ 解压hadoop并配置环境变量</h4><p>1、将上传来的hadoop解压到<code>/opt</code>文件夹下，并重命名为<code>hadoop</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># tar -zxvf hadoop-2.7.3.tar.gz -C /opt</div><div class="line"># mv hadoop-2.7.3 hadoop</div></pre></td></tr></table></figure></p>
<p>2、配置环境变量，以便能够在任何位置使用hadoop的相关命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># vim /etc/profile</div></pre></td></tr></table></figure></p>
<p>在末尾添加以下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">export HADOOP_HOME=/opt/hadoop</div><div class="line">export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</div><div class="line">export HADOOP_LOG_DIR=/home/hadoop/logs</div><div class="line">export YARN_LOG_DIR=$HADOOP_LOG_DIR</div></pre></td></tr></table></figure></p>
<p>让配置文件生效：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># source /etc/profile</div></pre></td></tr></table></figure></p>
<p>3、测试：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># which hadoop</div><div class="line">/opt/hadoop/bin/hadoop</div></pre></td></tr></table></figure></p>
<h4 id="§-建立hadoop的工作目录结构"><a href="#§-建立hadoop的工作目录结构" class="headerlink" title="§ 建立hadoop的工作目录结构"></a>§ 建立hadoop的工作目录结构</h4><p>本次选择在<code>/home</code>目录下建立hadoop的工作目录。目录结构为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">/home/hadoop</div><div class="line">/home/hadoop/tmp</div><div class="line">/home/hadoop/logs</div><div class="line">/home/hadoop/hdfs</div><div class="line">/home/hadoop/journal</div><div class="line">/home/hadoop/hdfs/datanode</div><div class="line">/home/hadoop/hdfs/namenode</div></pre></td></tr></table></figure></p>
<h4 id="§-修改hadoop配置文件，共7个"><a href="#§-修改hadoop配置文件，共7个" class="headerlink" title="§ 修改hadoop配置文件，共7个"></a>§ 修改hadoop配置文件，共7个</h4><p>所有的配置文件都在hadoop根目录下<code>hadoop/etc/hadoop</code>文件夹下面，所以要先<code>cd</code>到这个目录下。</p>
<p>1、配置<code>hadoop-env.sh</code>，将文件中的<code>export JAVA_HOME=${JAVA_HOME}</code>写成我们自己jdk的绝对路径：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># The java implementation to use.</div><div class="line">export JAVA_HOME=/opt/jdk8</div></pre></td></tr></table></figure></p>
<p>2、修改<code>yarn_env.sh</code>，将其中的<code>export JAVA_HOME=${JAVA_HOME}</code>写成自己的jdk的绝对路径：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># some Java parameters</div><div class="line">export JAVA_HOME=/opt/jdk8</div></pre></td></tr></table></figure></p>
<p>3、修改<code>core-site.xml</code>，在<code>&lt;configuration&gt;&lt;/configuration&gt;</code>中间加入如下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line"> &lt;!-- 指定hdfs的nameservice为ns1，这里的ns1对应于后面hdfs-site.xml中的dfs.nameservices标签的值 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</div><div class="line">    &lt;value&gt;hdfs://ns1&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line"> &lt;!-- 指定hadoop运行时产生文件的存储路径，这个路径也可以直接放到hadoop的根目录下--&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</div><div class="line">    &lt;value&gt;file:/home/hadoop/tmp&lt;/value&gt;</div><div class="line">  &lt;/property&gt;r</div><div class="line">&lt;!-- 指定zookeeper地址，多个用,分割,2181为客户端连接ZK服务器端口 --&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</div><div class="line">    &lt;value&gt;slaver1:2181,slaver2:2181,slaver3:2181&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>
<p>4、修改<code>hdfs-site.xml</code>，注意，在这个文件中有关于<code>journalnode</code>的配置信息，我们一般把<code>journalnode</code>部署到<code>slaver</code>节点上，并且只能为奇数，至少为3台（也可以是5、7、9等台数），在<code>&lt;configuration&gt;&lt;/configuration&gt;</code>中间加入如下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">    &lt;!-- dfs.nameservices 命名空间的逻辑名称，多个用,分割 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.nameservices&lt;/name&gt;</div><div class="line">      &lt;value&gt;ns1&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;!-- 指定ns1下有两个namenode，分别是nn1,nn2 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.ha.namenodes.ns1&lt;/name&gt;</div><div class="line">      &lt;value&gt;nn1,nn2&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;!--HDFS文件系统数据存储位置，可以分别保存到不同硬盘，突破单硬盘性能瓶颈，多个位置以逗号隔开--&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">     &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</div><div class="line">     &lt;value&gt;/home/hadoop/hdfs/datanode&lt;/value&gt;</div><div class="line">     &lt;final&gt;true&lt;/final&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">     &lt;!--NN存放元数据和日志位置--&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">     &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</div><div class="line">     &lt;value&gt;/home/hadoop/hdfs/namenode&lt;/value&gt;</div><div class="line">     &lt;final&gt;true&lt;/final&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;!-- 指定nn1的RPC通信地址 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.namenode.rpc-address.ns1.nn1&lt;/name&gt;</div><div class="line">      &lt;value&gt;master1:8020&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;!-- 指定nn1的HTTP通信地址 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.namenode.http-address.ns1.nn1&lt;/name&gt;</div><div class="line">      &lt;value&gt;master1:50070&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;!-- 指定nn2的RPC通信地址 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.namenode.rpc-address.ns1.nn2&lt;/name&gt;</div><div class="line">      &lt;value&gt;master2:8020&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;!-- 指定nn2的HTTP通信地址 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.namenode.http-address.ns1.nn2&lt;/name&gt;</div><div class="line">      &lt;value&gt;master2:50070&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;!-- 指定namenode的元数据存放的Journal Node的地址，必须奇数，至少三个 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</div><div class="line">      &lt;value&gt;qjournal://slaver1:8485;slaver2:8485;slaver3:8485/ns1&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;!--这是JournalNode进程保持逻辑状态的路径。这是在linux服务器文件的绝对路径--&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</div><div class="line">      &lt;value&gt;/home/hadoop/journal/&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;!-- 开启namenode失败后自动切换 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</div><div class="line">      &lt;value&gt;true&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;!-- 配置失败自动切换实现方式 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.client.failover.proxy.provider.ns1&lt;/name&gt;</div><div class="line">      &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;!-- 配置隔离机制方法，多个机制用换行分割 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</div><div class="line">      &lt;value&gt;</div><div class="line">        sshfence(hdfs)</div><div class="line">        shell(/bin/true)</div><div class="line">      &lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</div><div class="line">        &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;!-- 配置sshfence隔离机制超时时间30秒 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">       &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;</div><div class="line">       &lt;value&gt;30000&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>
<p>5、修改<code>mapred-site.xml</code>，首先需要将文件夹中的<code>mapred-site.xml.template</code>复制并重命名为<code>mapred-site.xml</code>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># cp mapred-site.xml.template mapred-site.xml</div></pre></td></tr></table></figure></p>
<p>在<code>&lt;configuration&gt;&lt;/configuration&gt;</code>中间加入如下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;!-- 通知框架MR使用YARN --&gt;</div><div class="line">    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</div><div class="line">    &lt;value&gt;yarn&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;!-- 配置 MapReduce JobHistory Server地址 ，默认端口10020 --&gt;</div><div class="line">    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</div><div class="line">    &lt;value&gt;0.0.0.0:10020&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;!-- 配置 MapReduce JobHistory Server HTTP地址， 默认端口19888 --&gt;</div><div class="line">    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</div><div class="line">    &lt;value&gt;0.0.0.0:19888&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>
<p>6、修改<code>yarn-site.xml</code>，在<code>&lt;configuration&gt;&lt;/configuration&gt;</code>中间加入如下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">        &lt;!--启用RM高可用--&gt;</div><div class="line">   &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</div><div class="line">        &lt;value&gt;true&lt;/value&gt;</div><div class="line">   &lt;/property&gt;</div><div class="line">        &lt;!--RM集群标识符--&gt;</div><div class="line">   &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</div><div class="line">        &lt;value&gt;rm-cluster&lt;/value&gt;</div><div class="line">   &lt;/property&gt;</div><div class="line">   &lt;property&gt;</div><div class="line">        &lt;!--指定两台RM主机名标识符--&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</div><div class="line">        &lt;value&gt;rm1,rm2&lt;/value&gt;</div><div class="line">   &lt;/property&gt;</div><div class="line">        &lt;!--RM故障自动切换--&gt;</div><div class="line">   &lt;property&gt;</div><div class="line">        &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.recover.enabled&lt;/name&gt;</div><div class="line">        &lt;value&gt;true&lt;/value&gt;</div><div class="line">   &lt;/property&gt;</div><div class="line">        &lt;!--RM故障自动恢复</div><div class="line">                &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; </div><div class="line">        &lt;value&gt;true&lt;/value&gt; </div><div class="line">   &lt;/property&gt; --&gt;</div><div class="line">        &lt;!--RM主机1，如果希望单独装在另外两个节点上，请写入对应的主机名，后面配置也需要相应修改--&gt;</div><div class="line">   &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</div><div class="line">        &lt;value&gt;master1&lt;/value&gt;</div><div class="line">   &lt;/property&gt;</div><div class="line">        &lt;!--RM主机2，如果希望单独装在另外两个节点上，请写入对应的主机名，后面配置也需要相应修改--&gt;</div><div class="line">   &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</div><div class="line">        &lt;value&gt;master2&lt;/value&gt;</div><div class="line">   &lt;/property&gt;</div><div class="line">       &lt;!--RM状态信息存储方式，一种基于内存(MemStore)，另一种基于ZK(ZKStore)--&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;</div><div class="line">       &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">        &lt;!--使用ZK集群保存状态信息--&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</div><div class="line">       &lt;value&gt;slaver1:2181,slaver2:2181,slaver3:2181&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">        &lt;!--向RM调度资源地址--&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt;</div><div class="line">        &lt;value&gt;master1:8030&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt;</div><div class="line">       &lt;value&gt;master2:8030&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">        &lt;!--NodeManager通过该地址交换信息--&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm1&lt;/name&gt;</div><div class="line">       &lt;value&gt;master1:8031&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm2&lt;/name&gt;</div><div class="line">       &lt;value&gt;master2:8031&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">        &lt;!--客户端通过该地址向RM提交对应用程序操作--&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt;</div><div class="line">       &lt;value&gt;master1:8032&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt;</div><div class="line">       &lt;value&gt;master2:8032&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">        &lt;!--管理员通过该地址向RM发送管理命令--&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.admin.address.rm1&lt;/name&gt;</div><div class="line">       &lt;value&gt;master1:8033&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.admin.address.rm2&lt;/name&gt;</div><div class="line">        &lt;value&gt;master2:8033&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">        &lt;!--RM HTTP访问地址,查看集群信息--&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;</div><div class="line">       &lt;value&gt;master1:8088&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">       &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;</div><div class="line">       &lt;value&gt;master2:8088&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">     &lt;!--不填这个运行自带的World Count可能会报错--&gt;</div><div class="line">    &lt;property&gt;  </div><div class="line">       &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;  </div><div class="line">       &lt;value&gt;mapreduce_shuffle&lt;/value&gt;  </div><div class="line">    &lt;/property&gt;      </div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure></p>
<p>7、修改<code>slaves</code>文件，这个文件中是写入希望成为datanode节点的机器的主机名：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># vim slaves</div><div class="line"></div><div class="line">slaver1</div><div class="line">slaver2</div><div class="line">slaver3</div></pre></td></tr></table></figure></p>
<h4 id="§-将配置好的hadoop拷贝到其他主机"><a href="#§-将配置好的hadoop拷贝到其他主机" class="headerlink" title="§ 将配置好的hadoop拷贝到其他主机"></a>§ 将配置好的hadoop拷贝到其他主机</h4><p>1、用<code>scp</code>命令向其他主机分发配置好的hadoop文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># cd /opt</div><div class="line"># scp -r ./hadoop master2:/opt</div><div class="line"># scp -r ./hadoop slaver1:/opt</div><div class="line"># scp -r ./hadoop slaver2:/opt</div><div class="line"># scp -r ./hadoop slaver3:/opt</div></pre></td></tr></table></figure></p>
<p>2、用<code>scp</code>命令向其他主机分发配置好的<code>/etc/profile</code>文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># scp /etc/profile master2:/etc</div><div class="line"># scp /etc/profile slaver1:/etc</div><div class="line"># scp /etc/profile slaver2:/etc</div><div class="line"># scp /etc/profile slaver3:/etc</div></pre></td></tr></table></figure></p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>此次分发会把三台<code>slaver</code>上的关于<code>zookeeper</code>的环境变量给覆盖掉，请重新配置。</li>
<li>如果不想重新配置<code>zookeeper</code>的环境变量，也可以手动复制<code>hadoop</code>的环境变量到每台机器上<code>/etc/profile</code>文件末尾，分别配置每台机器上的中<code>hadoop</code>的环境变量。</li>
</ul>
</blockquote>
<p>3、在每台主机上更新环境变量，使其生效：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># source /etc/profile</div></pre></td></tr></table></figure></p>
<h3 id="七、-启动hadoop集群"><a href="#七、-启动hadoop集群" class="headerlink" title="七、 启动hadoop集群"></a>七、 启动hadoop集群</h3><p>启动hadoop的高可用集群的时候，有启动顺序的限制：</p>
<p>1、在三台<code>slaver</code>节点上，分别启动<code>zookeeper</code>，前面已经做过，只需检查下<code>QuorumPeerMain</code>进程还在即可：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># jps</div><div class="line">21273 Jps</div><div class="line">13450 QuorumPeerMain</div></pre></td></tr></table></figure></p>
<p>2、在三台<code>slaver</code>节点上，分别启动<code>journalnode</code>进程，如果没有配置好hadoop的环境变量，可以到<code>hadoop</code>目录下的<code>sbin</code>目录下<code>./hadoop-daemon.sh start journalnode</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># hadoop-daemon.sh start journalnode</div></pre></td></tr></table></figure></p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>中间的<code>start</code>参数换成<code>stop</code>就成了停止指定进程的命令。</li>
</ul>
</blockquote>
<p>3、第一次运行要对HDFS(namenode)进行格式化，在<code>master1</code>上执行以下命令，如果没有配置好hadoop的环境变量，可以到<code>hadoop</code>目录下的<code>bin</code>目录下执行<code>./hdfs namenode –format</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># hdfs namenode -format</div><div class="line"># hadoop-daemon.sh start namenode</div></pre></td></tr></table></figure></p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>格式化第二次有可能会造成<code>DataNode</code>无法启动，原因是<code>NameSpaceID</code>不一致造成，解决方法是找出<code>dafs/datanode</code>目录下不一致的<code>VERSION</code>修改<code>NameSpaceID</code>，也可以尝试删除<code>hdfs/datanode</code>目录，重新格式化。</li>
</ul>
</blockquote>
<p>4、在<code>master2</code>使用以下命令，把<code>master2</code>节点的目录格式化并同步两个<code>master</code>节点的元数据，这个命令不会把<code>journalnode</code>目录再格式化了：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># hdfs namenode -bootstrapStandby</div><div class="line"># hadoop-daemon.sh start namenode</div></pre></td></tr></table></figure></p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>也可以在<code>master2</code>上重复上一条命令，重新格式化<code>NameNode</code>。</li>
</ul>
</blockquote>
<p>5、在<code>master1</code>上，执行以下命令格式化<code>ZK</code>，如果没有配置好hadoop的环境变量，可以到<code>hadoop</code>目录下的<code>bin</code>目录下执行<code>./hdfs zkfc –formatZK</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># hdfs zkfc -formatZK</div></pre></td></tr></table></figure></p>
<p>6、在<code>master1</code>和<code>master2</code>上，分别启动<code>ZKFC</code>来监控<code>NameNode</code>的状态，如果没有配置好hadoop的环境变量，可以到<code>hadoop</code>目录下的<code>sbin</code>目录下执行<code>./hadoop-daemon.sh start zkfc</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># hadoop-daemon.sh start zkfc</div></pre></td></tr></table></figure></p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>中间的<code>start</code>参数换成<code>stop</code>就成了停止指定进程的命令。</li>
</ul>
</blockquote>
<p>7、在<code>master1</code>上，启动<code>HDFS(NameNode)</code>，如果没有配置好hadoop的环境变量，可以到<code>hadoop</code>目录下的<code>sbin</code>目录下执行<code>./start-dfs.sh</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># start-dfs.sh</div></pre></td></tr></table></figure></p>
<p>8、在<code>master1</code>上，启动<code>YARN( ResourceManager)</code>，如果没有配置好hadoop的环境变量，可以到<code>hadoop</code>目录下的<code>sbin</code>目录下执行<code>./start-yarn.sh</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># start-yarn.sh</div></pre></td></tr></table></figure></p>
<p>9、在<code>master2</code>上启动<code>YARN( ResourceManager)</code>，如果没有配置好hadoop的环境变量，可以到<code>hadoop</code>目录下的<code>sbin</code>目录下执行<code>./yarn-daemon.sh start resourcemanager</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># yarn-daemon.sh start resourcemanager</div></pre></td></tr></table></figure></p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>中间的<code>start</code>参数换成<code>stop</code>就成了停止指定进程的命令。</li>
</ul>
</blockquote>
<p>10、如果在<code>master2</code>上<code>NameNode</code>没有正常启动，可以通过以下命令手动启动，如果没有配置好hadoop的环境变量，可以到<code>hadoop</code>目录下的<code>sbin</code>目录下执行<code>./yarn-daemon.sh start resourcemanager</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># hadoop-daemon.sh start namenode</div></pre></td></tr></table></figure></p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>中间的<code>start</code>参数换成<code>stop</code>就成了停止指定进程的命令。</li>
</ul>
</blockquote>
<p>11、验证hadoop集群（HDFS和YARN）是否正常启动，可以通过<code>jps</code>命令，查看各节点运行的<code>java</code>进程：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">// master1上运行的进程，前面的数字对应的进程的pid号</div><div class="line">977 ResourceManager</div><div class="line">32756 DFSZKFailoverController</div><div class="line">1364 Jps</div><div class="line">522 NameNode</div><div class="line"></div><div class="line">// master2上运行的进程，前面的数字对应的进程的pid号</div><div class="line">28164 NameNode</div><div class="line">28502 ResourceManager</div><div class="line">28633 Jps</div><div class="line">28059 DFSZKFailoverController</div><div class="line"></div><div class="line">// slaver1上运行的进程，前面的数字对应的进程的pid号</div><div class="line">13520 DataNode</div><div class="line">13876 Jps</div><div class="line">13367 JournalNode</div><div class="line">13690 NodeManager</div><div class="line">4523 QuorumPeerMain</div><div class="line"></div><div class="line">// slaver2上运行的进程，前面的数字对应的进程的pid号</div><div class="line">21744 DataNode</div><div class="line">22101 Jps</div><div class="line">21591 JournalNode</div><div class="line">13450 QuorumPeerMain</div><div class="line">21915 NodeManager</div><div class="line"></div><div class="line">// slaver3上运行的进程，前面的数字对应的进程的pid号</div><div class="line">22898 DataNode</div><div class="line">23253 Jps</div><div class="line">22744 JournalNode</div><div class="line">17210 QuorumPeerMain</div><div class="line">23069 NodeManager</div></pre></td></tr></table></figure></p>
<p>12、通过hadoop的web页面，查看是否异常，以及高可用是否正常运行：<br>在浏览器中输入：<a href="http://master1:50070" target="_blank" rel="external">http://master1:50070</a> 其中如果本地<code>hosts</code>中没有建立相应的主机名与IP的映射关系，将<code>master1</code>换成相应的IP，显示如下：</p>
<p><img src="http://img.blog.csdn.net/20161017205706993" alt="master1"></p>
<p>在浏览器中输入：<a href="http://master2:50070" target="_blank" rel="external">http://master2:50070</a> 其中如果本地<code>hosts</code>中没有建立相应的主机名与IP的映射关系，将<code>master2</code>换成相应的IP，显示如下：</p>
<p><img src="http://img.blog.csdn.net/20161017205846199" alt="master2"></p>
<p><strong>由以上两个图可以看出，<code>master1</code>出于<code>active</code>状态，<code>master2</code>处于<code>standby</code>状态，说明HDFS的HA已成功建立。</strong> </p>
<p>在浏览器中输入：<a href="http://master1:8088/cluster/cluster" target="_blank" rel="external">http://master1:8088/cluster/cluster</a> 其中如果本地<code>hosts</code>中没有建立相应的主机名与IP的映射关系，将<code>master1</code>换成相应的IP，显示如下：</p>
<p><img src="http://img.blog.csdn.net/20161017210427255" alt="yarn1"></p>
<p>在浏览器中输入：<a href="http://master2:8088/cluster/cluster" target="_blank" rel="external">http://master2:8088/cluster/cluster</a> 其中如果本地<code>hosts</code>中没有建立相应的主机名与IP的映射关系，将<code>master2</code>换成相应的IP，显示如下：</p>
<p><img src="http://img.blog.csdn.net/20161017210617617" alt="yarn1"></p>
<p><strong>由以上两个图可以看出，<code>master1</code>出于<code>active</code>状态，<code>master2</code>处于<code>standby</code>状态，说明<code>ResourceManager</code>的HA也已成功建立。</strong></p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>如果是在Openstack虚拟机上搭建的hadoop，那么使用对应主机的IP的时候，要用每台主机对应的<code>floating ip</code>，而不是openstack的内部私网IP，比如开始的172.16.1.33，就是openstack内部的私网IP。</li>
<li>可以通过<code>kill</code>命令后面跟相应进程的pid号来停掉<code>active</code>节点的相应进程，模拟故障状态，观察HA的迁移，但注意关掉后，再单独启动起来。</li>
</ul>
</blockquote>
<p>13、停止hadoop集群，可以通过执行以下命令来停止hadoop集群，如果没有配置好hadoop的环境变量，可以到<code>hadoop</code>目录下的<code>sbin</code>目录下执行<code>./stop-all.sh</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># stop-all.sh</div></pre></td></tr></table></figure></p>
<blockquote>
<p><strong>NOTE:</strong></p>
<ul>
<li>这种方法并不能够停掉<code>zookeeper</code>相关进程和<code>master2</code>上单独启动的<code>ResourceManager</code>进程，如有必要，这些需要另外手动停掉。</li>
</ul>
</blockquote>
<h3 id="八、总结"><a href="#八、总结" class="headerlink" title="八、总结"></a>八、总结</h3><p>Hadoop的高可用集群部署并不是特别困难，但是配置内容比单<code>NameNode</code>节点的要多的多，且比较复杂，应当细心。另外，最好多了解下这些配置文件里面各个项所表示的含义，比较好的方法是仔细研读一下相应版本的官方文档，也方便对相应细节做出优化。</p>
<h3 id="九、附：HDFS操作命令"><a href="#九、附：HDFS操作命令" class="headerlink" title="九、附：HDFS操作命令"></a>九、附：HDFS操作命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">// 查看DataNode节点信息，可以使用这个命令脚本监控DFS状况</div><div class="line"># hadoop dfsadmin -report  </div><div class="line">// 指定HDFS地址访问</div><div class="line"># hadoop fs -ls hdfs://hcluster:9000/    </div><div class="line">// 列出HDFS文件系统目录下文件和目录</div><div class="line"># hadoop fs -ls /   </div><div class="line">// 递归列出目录</div><div class="line"># hadoop fs -lsr /  </div><div class="line">// 创建test目录</div><div class="line"># hadoop fs -mkdir /test  </div><div class="line">// 上传文件到test目录</div><div class="line"># hadoop fs -put /root/test.txt /test/test.txt  </div><div class="line">// 查看文件内容</div><div class="line"># hadoop fs -cat /test/test.txt  </div><div class="line">// 查看文件大小</div><div class="line"># hadoop fs -du /test/test.txt  </div><div class="line">// 删除文件 </div><div class="line"># hadoop fs -rm /test/test.txt   </div><div class="line">// 递归删除目录或文件</div><div class="line"># hadoop fs -rmr /test</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍如何搭建HDFS（NameNode）和ResourceManager高可用的Hadoop集群。&lt;/p&gt;
    
    </summary>
    
      <category term="学习" scheme="https://www.hanbert.cn/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Hadoop" scheme="https://www.hanbert.cn/tags/Hadoop/"/>
    
      <category term="高可用" scheme="https://www.hanbert.cn/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    
      <category term="集群部署" scheme="https://www.hanbert.cn/tags/%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
    
  </entry>
  
  <entry>
    <title>Cloud Foundry 部署应用的过程及问题</title>
    <link href="https://www.hanbert.cn/2016/06/11/Cloud-Foundry-%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8%E7%9A%84%E8%BF%87%E7%A8%8B%E5%8F%8A%E9%97%AE%E9%A2%98/"/>
    <id>https://www.hanbert.cn/2016/06/11/Cloud-Foundry-部署应用的过程及问题/</id>
    <published>2016-06-11T00:23:07.000Z</published>
    <updated>2017-04-10T02:47:38.392Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、Cloud-Foundry-简介"><a href="#一、Cloud-Foundry-简介" class="headerlink" title="一、Cloud Foundry 简介"></a>一、Cloud Foundry 简介</h3><p>Cloud Foundry是VMware推出的业界第一个开源PaaS云平台，它支持多种框架、语言、运行时环境、云平台及应用服务，使开发人员能够在几秒钟内进行应用程序的部署和扩展，无需担心任何基础架构的问题。同时，它本身是一个基于Ruby on Rails的由多个相对独立的子系统通过消息机制组成的分布式系统，使平台在各层级都可水平扩展，既能在大型数据中心里运行，也能运行在一台桌面电脑中，二者使用相同的代码库。</p>
<p>作为新一代云应用平台，Cloud Foundry专为私有云计算环境、企业级数据中心和公有云服务提供商所打造。Cloud Foundry云平台可以简化现代应用程序的开发、交付和运行过程，在面对多种公有云和私有云选择、符合业界标准的高效开发框架以及应用基础设施服务时，可以显著提高开发者在云环境中部署和运行应用程序的能力。</p>
<a id="more"></a>
<h3 id="二、自定义buildpack包"><a href="#二、自定义buildpack包" class="headerlink" title="二、自定义buildpack包"></a>二、自定义buildpack包</h3><h4 id="§-Buildpack包的简介"><a href="#§-Buildpack包的简介" class="headerlink" title="§ Buildpack包的简介"></a>§ Buildpack包的简介</h4><p>CF默认集成的buildpack包：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Name</th>
<th style="text-align:left">Supported Languages and Frameorks</th>
<th style="text-align:left">GitHub Repo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Java</td>
<td style="text-align:left">Grails, Play, Spring, or any other JVM-based language or framework</td>
<td style="text-align:left"><a href="https://github.com/cloudfoundry/java-buildpack" target="_blank" rel="external">Java Source</a></td>
</tr>
<tr>
<td style="text-align:left">Ruby</td>
<td style="text-align:left">JRuby, Rack, Rails, or Sinatra</td>
<td style="text-align:left"><a href="https://github.com/cloudfoundry/ruby-buildpack" target="_blank" rel="external">Ruby source</a></td>
</tr>
<tr>
<td style="text-align:left">Node.js</td>
<td style="text-align:left">Node or JavaScript</td>
<td style="text-align:left"><a href="https://github.com/cloudfoundry/nodejs-buildpack" target="_blank" rel="external">Node.js source</a></td>
</tr>
<tr>
<td style="text-align:left">Binary</td>
<td style="text-align:left">N/A</td>
<td style="text-align:left"><a href="https://github.com/cloudfoundry/binary-buildpack" target="_blank" rel="external">Binary source</a></td>
</tr>
<tr>
<td style="text-align:left">Go</td>
<td style="text-align:left">N/A</td>
<td style="text-align:left"><a href="https://github.com/cloudfoundry/go-buildpack" target="_blank" rel="external">Go source</a></td>
</tr>
<tr>
<td style="text-align:left">PHP</td>
<td style="text-align:left">Cake, Symfony, Zend, Nginx, or HTTPD</td>
<td style="text-align:left"><a href="https://github.com/cloudfoundry/php-buildpack" target="_blank" rel="external">PHP source</a></td>
</tr>
<tr>
<td style="text-align:left">Python</td>
<td style="text-align:left">Django or Flask</td>
<td style="text-align:left"><a href="https://github.com/cloudfoundry/python-buildpack" target="_blank" rel="external">Python source</a></td>
</tr>
<tr>
<td style="text-align:left">Staticfile</td>
<td style="text-align:left">HTML, CSS, JavaScript, or Nginx</td>
<td style="text-align:left"><a href="Staticfile source">Staticfile source</a></td>
</tr>
</tbody>
</table>
<p>这只是CF默认自带的几种，理论上，只要能做出buildpack包，任何语言都可以正常在CF上运行。</p>
<blockquote>
<p><strong>More Buildpacks：</strong></p>
<ul>
<li><a href="https://github.com/cloudfoundry-community/cf-docs-contrib/wiki/Buildpacks" target="_blank" rel="external">Cloud Foundry Community Buildpack</a></li>
<li><a href="https://devcenter.heroku.com/articles/third-party-buildpacks" target="_blank" rel="external">Heroku Third-Party Buildpack</a></li>
<li>Create your own buildpack: <a href="https://docs.cloudfoundry.org/buildpacks/custom.html" target="_blank" rel="external">Custom Buildpack</a></li>
</ul>
</blockquote>
<p>一般buildpack包只包含的基础环境和简单的脚本设置，在使用的时候才会根据设置，从网上的源自己获取相应的 dependencies ，但大部分的源，国内访问都不是很方便，经常会出现无法下载相应的 dependencies 而导致 staging 失败。所以，国内想正常使用CF基本需要 offline 的 buildpack 包，而目前网络上提供的纯 offline 的 buildpack 比较少。</p>
<blockquote>
<p><strong>For more information about custom buildpacks:</strong></p>
<ul>
<li><a href="https://docs.cloudfoundry.org/buildpacks/custom.html" target="_blank" rel="external">Custom Buildpacks</a></li>
<li><a href="https://docs.cloudfoundry.org/buildpacks/depend-pkg-offline.html" target="_blank" rel="external">Packaging Dependencies for Offline Buildpacks</a></li>
<li><a href="https://docs.cloudfoundry.org/buildpacks/supported-binary-dependencies.html" target="_blank" rel="external">Supported Binary Dependencies</a></li>
</ul>
</blockquote>
<p>就目前我们所能找到的 buildpack 包来看，都是用 Ruby 写的，所以，如果你想全新创建一个 buildpack 包，不仅要了解buildpack 的构成和逻辑，还需要懂 Ruby 编程知识。所以，目前大家首选的自定义方法是，在现有的 buildpack 包的基础之上，根据自己的实际需求进行修改，并离线所有的 dependencies 或者建立自己的本地源。</p>
<p>另外，从 github 上下载 buildpack 包，基本不能直接使用，需要经过以下步骤处理：</p>
<ul>
<li>Make sure you have fetched submodules</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git submodule update --init</div></pre></td></tr></table></figure>
<ul>
<li>Get latest buildpack dependencies</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">BUNDLE_GEMFILE=cf.Gemfile bundle</div></pre></td></tr></table></figure>
<ul>
<li>Build the buildpack</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">BUNDLE_GEMFILE=cf.Gemfile bundle exec buildpack-packager [ --cached | --uncached ]</div></pre></td></tr></table></figure>
<ul>
<li>Use in Cloud Foundry</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cf create-buildpack CUSTOM_BUILDPACK_NAME CUSTOM_BUILDPACK_NAME.zip 1</div><div class="line">cf push my_app -b CUSTOM_BUILDPACK_NAME</div></pre></td></tr></table></figure>
<ul>
<li>Java buildpack 的编译和打包操作和以上有些不一样：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">bundle install</div><div class="line">bundle exec rake package OFFLINE=true PINNED=true</div><div class="line">...</div><div class="line">Creating build/java-buildpack-offline-cfd6b17.zip</div></pre></td></tr></table></figure>
<blockquote>
<p><strong><em>NOTE:</em></strong></p>
<ul>
<li>本地电脑上要事先安装 zip 工具</li>
<li>本地电脑上要事先安装 Ruby 语言并配置好环境变量</li>
<li>本地电脑上要事先安装 Bundler 工具</li>
<li>事先学会 Bundler 的基本使用，参考 <em><a href="http://bundler.io/" target="_blank" rel="external">bundler.io</a></em></li>
<li>由于 Bundler 的源是国外的，连接会不稳定，断开后，可以重新运行命令继续</li>
</ul>
</blockquote>
<h4 id="§-基本原理"><a href="#§-基本原理" class="headerlink" title="§ 基本原理"></a>§ 基本原理</h4><p>CF运行应用的基本过程是将用户发布的应用程序包解压开，然后将自己的所有buildpack拿来，按照指定顺序与程序包进行匹配，直到找到第一个能够运行这些代码的buildpack，然后将buildpack也解开，与这些应用代码打成一个包（即droplet），在按照指定的运行环境参数生成容器，将droplet扔进去，按照buildpack指定的启动命令，启动应用。<br>在上面的过程中，buildpack做了三个动作：</p>
<ul>
<li><strong>detect:</strong> 检查当前应用程序包是否能够用本buildpack支持运行，比如，java buildpack发现WEB-INF路径就认为自己能够运行它。 </li>
<li><strong>compile:</strong> 这是buildpack的核心文件，一般作用就是去拉取相应的Runtime（e.g. python2.7/ruby1.9.3）下来，做一下配置放到指定位置，拉取相应的Framework（e.g. Flask/Django）下来，做一下配置，放到指定位置。将应用程序包与buildpack包水乳交融一下，比如将java程序包放到tomcat的应用目录下，然后替换某些参数，比如将当前dea里的随机端口赋予这个tomcat实例。 </li>
<li><strong>release:</strong> 将droplet启动，比如运行tomcat的startup.sh。 </li>
</ul>
<blockquote>
<p>任何一个buildpack都有一个bin路径，放着三个指定名字（detect、compile、release）的脚本（任何dea的os能执行的脚本都可以），然后具体的实现逻辑就从这里触发了。</p>
</blockquote>
<h4 id="§-自定义-buildpack-包的样例"><a href="#§-自定义-buildpack-包的样例" class="headerlink" title="§ 自定义 buildpack 包的样例"></a>§ 自定义 buildpack 包的样例</h4><p> 下面我们自定义一个buildpack来支持java web项目。</p>
<ul>
<li><code>bin/detect</code>脚本文件 </li>
</ul>
<p><strong>A. bash 版</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env bash </div><div class="line"># cf 会给detect脚本传入一个参数，即 build dir，里边就是那一坨app散文件</div><div class="line">BUILD_DIR=$1</div><div class="line"># 既然是java web的项目，而且跑在tomcat上，我们没别的要求，只要web.xml位于规范位置即可</div><div class="line">if [ -d &quot;$&#123;BUILD_DIR&#125;/WEB-INF&quot; ] &amp;&amp; [ -f &quot;$&#123;BUILD_DIR&#125;/WEB-INF/web.xml&quot; ]; then</div><div class="line"># 按照惯例，一般会把所侦测到的项目类型echo出来，</div><div class="line"># 这个echo出来的字符串和compile、release脚本没有半毛钱关系</div><div class="line">echo &quot;JavaWeb&quot;</div><div class="line"># 返回0表示detect成功执行，cf会继续执行compile，返回非0值cf就不继续往下走了</div><div class="line">exit 0</div><div class="line">fi</div><div class="line"></div><div class="line">echo &quot;no&quot;</div><div class="line">exit 1</div></pre></td></tr></table></figure>
<p><strong>B. ruby 版</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env ruby</div><div class="line"></div><div class="line">gemfile_path = File.join ARGV[0], &quot;Gemfile&quot;</div><div class="line"></div><div class="line">if File.exist?(gemfile_path)</div><div class="line">  puts &quot;Ruby&quot;</div><div class="line">  exit 0</div><div class="line">else</div><div class="line">  exit 1</div><div class="line">end</div></pre></td></tr></table></figure>
<ul>
<li><code>bin/compile</code> 脚本文件</li>
</ul>
<p>compile 文件做的事情稍微复杂一些，也是三个脚本文件中任务最重的一个。如果觉得 bash 脚本语言不能够很好的胜任，可以换用其他任何可以运行的脚本语言，比如 Ruby。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"># detect、compile、release这几个文件都没有后缀，用你喜欢的语言就好，这里是用的ruby</div><div class="line">$stdout.sync = true</div><div class="line"># 自己创建了一个lib目录，塞入环境变量，</div><div class="line"># 如之前所述，除了bin目录必须固定之外，其他文件（夹）随你喜欢来安排</div><div class="line">$:.unshift File.expand_path(&apos;../../lib&apos;, __FILE__)</div><div class="line">require &apos;global&apos;</div><div class="line">require &apos;main_pack&apos;</div><div class="line">require &apos;fileutils&apos;</div><div class="line"> </div><div class="line">build_path = ARGV[0]</div><div class="line">cache_path = ARGV[1]</div><div class="line">FileUtils.mkdir_p(build_path)</div><div class="line">FileUtils.mkdir_p(cache_path)</div><div class="line"># 主要逻辑都放在MainPack里了，无非就是下载、配置</div><div class="line">pack = MainPack.new(Global.new(build_path, cache_path))</div><div class="line">pack.compile</div></pre></td></tr></table></figure>
<blockquote>
<p><strong><em>NOTE:</em></strong></p>
<ol>
<li>为了提高下载速度，免受网络困扰，同时增加掌控力度，一般会把依赖的tar包放到内网某个位置去自己管理</li>
<li><code>build_dir/.profile.d/*.sh</code>文件都会在运行app之前由 CloudFoundry 帮我们提前运行，那在此导出一些环境变量之类的就比较方便了</li>
</ol>
</blockquote>
<ul>
<li><code>bin/release</code> 脚本文件</li>
</ul>
<p>此处的 release 脚本文件同样是用ruby写的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env ruby</div><div class="line"></div><div class="line">require &apos;yaml&apos;</div><div class="line"></div><div class="line">yml = &#123;</div><div class="line">&apos;addons&apos; =&gt; [],</div><div class="line">&apos;config_vars&apos; =&gt; &#123;&#125;,</div><div class="line">&apos;default_process_types&apos; =&gt; &#123;</div><div class="line">&apos;web&apos; =&gt; &apos;./bin/catalina.sh run&apos;</div><div class="line">&#125;</div><div class="line">&#125;.to_yaml</div><div class="line"></div><div class="line">puts yml</div></pre></td></tr></table></figure>
<blockquote>
<p><code>default_process_types</code>字段下面的web字段的值就是告诉CloudFoundry如何启动这个app，当然，我们可以在push应用的时候覆盖这个配置。</p>
</blockquote>
<p>buildpack 中提供的语言环境和 runtime（比如 php、apache），一般有多个版本可选，如果默认版本不符合用户需求，用户可以在自己程序的某个指定文件中配置自己所需要的版本，比如，用户程序中有个<code>config/dependency.yml</code>，如果用户提供了此文件，用用户指定的版本，如果用户没有提供，那就用 buildpack 默认的版本。</p>
<p>CloudFoundry 有一个内部的仓储（里边是python、jdk、apache、php之类的buildpack需要的），版本管理由用户自己介入通过用户程序中的<code>config/dependency.yml</code>作为媒介，用户可以自由的做版本的管理。</p>
<p>而有些不太好搞的依赖，比如某个python webapp要求系统事先安装flask1.0，推荐做法是，用户自己把flask打包到自己的程序中一起上传（这样用户发布出来的包就已经是没有依赖的)；也可以在根目录下提供一个<code>requirements.txt</code>，里边写上要安装的库，启动之前用<code>pip</code>来自动安装。如果有些依赖仍然搞不定，用户可以在启动脚本中写一些自定义命令。</p>
<p>总体来看，buildpack 帮助 paas 平台完成了一个 app 在部署层面的抽象，主要搞定 app 依赖的 runtime 和 framework ，相当于搞定了静态依赖。操作系统特性通过统一定制 rootfs 来搞，不在 buildpack 问题域。配置文件差异问题，buildpack 认为一个app 一套配置，上层怎么处理，开发人员自己说了算，可以搭建多套 CloudFoundry 或者创建不同的 app：dev-app/prod-app之类；运行时依赖也需要开发人员自己搞定，提前确认好相应的依赖是否已经在 run，版本是否OK之类。</p>
<h4 id="§-当前系统中的-Buildpack-版本"><a href="#§-当前系统中的-Buildpack-版本" class="headerlink" title="§ 当前系统中的 Buildpack 版本"></a>§ 当前系统中的 Buildpack 版本</h4><p>本次在尝试自定义 Buildpack 包的过程中，还对当前Cloud Foundry 系统中的Buildpack做了全面的更新。目前的已有的Buildpack的版本如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">buildpack              position   enabled   locked   filename</div><div class="line">staticfile_buildpack   1          true      false    staticfile_buildpack-cached-v1.3.10.zip</div><div class="line">ruby_buildpack         2          true      false    ruby_buildpack-cached-v1.6.20.zip</div><div class="line">nodejs_buildpack       3          true      false    nodejs_buildpack-cached-v1.5.18.zip</div><div class="line">go_buildpack           4          true      false    go_buildpack-cached-v1.7.11.zip</div><div class="line">python_buildpack       5          true      false    python_buildpack-cached-v1.5.8.zip</div><div class="line">php_buildpack          6          true      false    php_buildpack-cached-v4.3.17.zip</div><div class="line">binary_buildpack       7          true      false    binary_buildpack-cached-v1.0.3.zip</div><div class="line">java_buildpack         8          true      false    java-buildpack-offline-v3.8.1_4.zip</div></pre></td></tr></table></figure>
<p>这是目前Pivotal官网所能提供的最新版。</p>
<blockquote>
<p><strong><em>NOTE:</em></strong><br>可以从<em><a href="https://network.pivotal.io" target="_blank" rel="external">network.pivotal.io</a></em>下载 Pivotal 相关的产品。<br>由于Pivotal官网服务器不在国内，所以，下载东西如果100MB+需要有点耐心，可能出现频繁下载失败的情况。<br>这个时候，只需要反复下载，尽量选择晚上下载，网速能好一点。<br>比如下图下载Python的Buildpack，反反复复下载了三天，侥幸成功!</p>
<p><center><strong><em>有时候不努力一把你就不知道什么叫做绝望！</em></strong></center><br><img src="http://upload-images.jianshu.io/upload_images/2311911-2e035399182281f8.png" alt="1|center"></p>
</blockquote>
<h3 id="三、部署应用"><a href="#三、部署应用" class="headerlink" title="三、部署应用"></a>三、部署应用</h3><h4 id="§-Java-Buildpack-的使用"><a href="#§-Java-Buildpack-的使用" class="headerlink" title="§ Java Buildpack 的使用"></a>§ Java Buildpack 的使用</h4><p>Java Buildpack 可以用来部署 Grails, Play, Spring 或者其他任何基于JVM的语言或者框架。JVM类的应用程序，在CloudFoundry上部署之前需要事先在本地打包，所谓的打包，就是把用到的类工具、资源（图片等）、源码、编译后的文件等全都放到一起，以达到发布、管理、修改、维护方便的目的。</p>
<blockquote>
<p><strong>包的种类：</strong></p>
<ul>
<li>JAR包：打成JAR包的代码，一般作为工具类，在项目中，会应用到N多JAR工具包；</li>
<li>WAR包：JAVA WEB工程，都是打成WAR包，进行发布，如果我们的服务器选择TOMCAT等轻量级服务器，一般就打出WAR包进行发布；</li>
<li>EAR包：这针对企业级项目的，实际上EAR包中包含WAR包和几个企业级项目的配置文件而已，一般服务器选择WebSphere等，都会使用EAR包。</li>
</ul>
</blockquote>
<p>对于不同的基于JVM的语言或者框架，其打包的方式略有不同。</p>
<ul>
<li><strong>Grails</strong></li>
</ul>
<p>Grails packages applications into WAR files for deployment into a Servlet container. To build the WAR file and deploy it, run the following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ grails prod war</div><div class="line">$ cf push my-application; -p target/my-application-version.war</div></pre></td></tr></table></figure>
<ul>
<li><strong>Groovy</strong></li>
</ul>
<p>Groovy applications based on both <code>Ratpack</code> and a simple collection of files are supported.</p>
<ul>
<li><strong>Ratpack</strong></li>
</ul>
<p>Ratpack packages applications into two different styles; Cloud Foundry supports the <code>distZip</code> style. To build the ZIP and deploy it, run the following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ gradle distZip</div><div class="line">$ cf push my-application -p build/distributions/my-application.zip</div></pre></td></tr></table></figure>
<ul>
<li><strong>Raw Groovy</strong></li>
</ul>
<p>Groovy applications that are made up of a <code>single entry point</code> plus any supporting files can be run without any other work. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ cf push my-application</div></pre></td></tr></table></figure>
<ul>
<li><strong>Java Main</strong></li>
</ul>
<p>Java applications with a <code>main()</code>method can be run provided that they are packaged as <code>self-executable JARs</code>.</p>
<blockquote>
<p><strong><em>NOTE:</em></strong><br>If your application is not <code>web-enabled</code>, you must suppress route creation to avoid a <code>“failed to start accepting connections”</code>error. To suppress route creation, add <code>no-route: true</code> to the application <code>manifest.yml</code> or use the <code>--no-route</code> flag with the <code>cf push</code> command.</p>
</blockquote>
<ul>
<li><strong>Maven</strong></li>
</ul>
<p>A Maven build can create a <code>self-executable JAR</code>. To build and deploy the JAR, run the following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ mvn package</div><div class="line">$ cf push my-application -p target/my-application-version.jar</div></pre></td></tr></table></figure>
<ul>
<li><strong>Gradle</strong></li>
</ul>
<p>A Gradle build can create a self-executable JAR. To build and deploy the JAR, run the following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ gradle build</div><div class="line">$ cf push my-application -p build/libs/my-application-version.jar</div></pre></td></tr></table></figure>
<ul>
<li><strong>Play Framework</strong></li>
</ul>
<p>The Play Framework packages applications into two different styles. Cloud Foundry supports both the <code>staged</code> and <code>dist</code> styles. To build the dist style and deploy it, run the following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ play dist</div><div class="line">$ cf push my-application -p target/universal/my-application-version.zip</div></pre></td></tr></table></figure>
<ul>
<li><strong>Spring Boot CLI</strong></li>
</ul>
<p>Spring Boot can run applications comprised entirely of POGOs. To deploy then, run the following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ spring grab *.groovy</div><div class="line">$ cf push my-application</div></pre></td></tr></table></figure>
<ul>
<li><strong>Servlet</strong></li>
</ul>
<p>Java applications can be packaged as Servlet applications.</p>
<ul>
<li><strong>Maven</strong></li>
</ul>
<p>A Maven build can create a Servlet WAR. To build and deploy the WAR, run the following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ mvn package</div><div class="line">$ cf push my-application -p target/my-application-version.war</div></pre></td></tr></table></figure>
<ul>
<li><strong>Gradle</strong></li>
</ul>
<p>A Gradle build can create a Servlet WAR. To build and deploy the JAR, run the following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ gradle build</div><div class="line">$ cf push my-application -p build/libs/my-application-version.war</div></pre></td></tr></table></figure>
<blockquote>
<p><strong><em>NOTE:</em></strong><br>在上面使用各种工具打包的过程中，经常会遇到，需要安装本地环境或各种工具。所以，如果遇到不能正常打包的情况，最好仔细看看它的错误提示，根据错误提示，看是本地环境问题，还是在线安装的时候，网络出了问题，如果是网络出了问题，能不能手动根据其下载地址，把工具或者依赖下下来，手动安装或者建立本地仓库，然后更新其配置文件中相应的下载链接等等。当然，由此带来的问题是，可能打包时间过长，耐心等待。<br>总而言之，这个过程基本上不会一帆风顺，需要比较强的Troubleshooting的能力。</p>
</blockquote>
<h4 id="§-Ruby-Buildpack-的使用"><a href="#§-Ruby-Buildpack-的使用" class="headerlink" title="§ Ruby  Buildpack 的使用"></a>§ Ruby  Buildpack 的使用</h4><p>Ruby Buildpack 包可以用来部署Rack, Rails, or Sinatra应用。</p>
<p>在部署应用之前，首先要做的是，用<code>Bundler</code>工具在你的应用文件夹下面创建<code>Gemfile</code>和<code>Gemfile.lock</code>文件。</p>
<p>Bundler的安装和使用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ gem update --system</div><div class="line">$ gem install bundler</div><div class="line">$ bundle install</div><div class="line">$ git add Gemfile Gemfile.lock</div></pre></td></tr></table></figure>
<p><strong>部署一个Ruby的应用：</strong></p>
<blockquote>
<p>Prerequisites</p>
<ul>
<li>A Ruby 2.x application that runs locally on your workstation</li>
<li>Bundler configured on your workstation</li>
<li>Basic to intermediate Ruby knowledge</li>
<li>The cf Command Line Interface (CLI) installed on your workstation</li>
</ul>
</blockquote>
<ul>
<li>Clone the pong_matcher_ruby app from GitHub：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/cloudfoundry-samples/pong_matcher_ruby.git</div></pre></td></tr></table></figure>
<blockquote>
<p><strong><em>Note: </em></strong>Ensure that your Ruby app runs locally before continuing with this procedure.</p>
</blockquote>
<ul>
<li>Create and Bind a Service Instance for a Ruby Application<br>Create a Service Instance</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># view services and plans that are available to you</div><div class="line">$ cf marketplace</div><div class="line">#cf create-service SERVICE PLAN SERVICE_INSTANCE</div><div class="line">$ cf create-service rediscloud 30mb redis</div><div class="line">Creating service redis in org Cloud-Apps / space development as clouduser@example.com....</div><div class="line">OK</div></pre></td></tr></table></figure>
<p>Bind a Service Instance</p>
<p>When you bind an app to a service instance, Cloud Foundry writes information about the service instance to the VCAP_SERVICES app environment variable. The app can use this information to integrate with the service instance.</p>
<p>You can bind a service to an application with the command :</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ cf bind-service APPLICATION SERVICE_INSTANCE</div></pre></td></tr></table></figure>
<p>Alternately, you can configure the deployment <code>manifest</code> file by adding a <code>services</code> block to the applications block and specifying the service instance. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">services:</div><div class="line">  - redis</div></pre></td></tr></table></figure>
<ul>
<li>Log in and Target the API Endpoint</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ cf login -a API_ENDPOINT</div></pre></td></tr></table></figure>
<ul>
<li>Deploy an App</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ cf push pong_matcher_ruby -n HOST_NAME</div></pre></td></tr></table></figure>
<blockquote>
<p>通过修改<code>manifest.yml</code> 文件，能够有效地控制部署应用的过程，包括instances、memory、command等 </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">---</div><div class="line">applications:</div><div class="line">  - name: pong</div><div class="line">    memory: 256M</div><div class="line">    instances: 1</div><div class="line">    path: .</div><div class="line">    command: ruby server.rb</div><div class="line"></div><div class="line"># code_snippet ruby-2b start</div><div class="line">    services:</div><div class="line">      - redis</div><div class="line"># code_snippet ruby-2b end</div></pre></td></tr></table></figure>
<h4 id="§-Node-js-Buildpack-的使用"><a href="#§-Node-js-Buildpack-的使用" class="headerlink" title="§ Node.js Buildpack 的使用"></a>§ Node.js Buildpack 的使用</h4><p>Cloud Foundry expects a <code>package.json</code> in your Node.js app. You can specify the version of Node.js you want to use in the <code>engine</code> node of your <code>package.json</code> file.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  &quot;name&quot;: &quot;first&quot;,</div><div class="line">  &quot;version&quot;: &quot;0.0.1&quot;,</div><div class="line">  &quot;author&quot;: &quot;Demo&quot;,</div><div class="line">  &quot;dependencies&quot;: &#123;</div><div class="line">    &quot;express&quot;: &quot;3.4.8&quot;,</div><div class="line">    &quot;consolidate&quot;: &quot;0.10.0&quot;,</div><div class="line">    &quot;express&quot;: &quot;3.4.8&quot;,</div><div class="line">    &quot;swig&quot;: &quot;1.3.2&quot;</div><div class="line">  &#125;,</div><div class="line">  &quot;engines&quot;: &#123;</div><div class="line">    &quot;node&quot;: &quot;0.12.7&quot;,</div><div class="line">    &quot;npm&quot;: &quot;2.7.4&quot;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>You must use the PORT environment variable to determine which port your app should listen on. In order to also run your app locally, you may want to make port 3000 the default:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">app.listen(process.env.PORT || 3000);</div></pre></td></tr></table></figure>
<p>Node.js apps require a start command. You can specify the web start command for a Node.js app in a <code>Procfile</code> or in the app deployment <code>manifest</code>. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">---</div><div class="line">applications:</div><div class="line">- name: my-app</div><div class="line">  command: node my-app.js //start command</div><div class="line">... the rest of your settings ...</div></pre></td></tr></table></figure>
<p>Alternately, specify the start command with <code>cf push -c</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ cf push my-app -c &quot;node my-app.js&quot;</div></pre></td></tr></table></figure>
<blockquote>
<p><strong>Application Bundling</strong><br>You do not need to run <code>npm install</code> before deploying your app. Cloud Foundry runs it for you when your app is pushed. If you prefer to run <code>npm install</code> and create a <code>node_modules</code> folder inside of your app, this is also supported.</p>
</blockquote>
<h4 id="§-Binary-Buildpack-的使用"><a href="#§-Binary-Buildpack-的使用" class="headerlink" title="§ Binary Buildpack 的使用"></a>§ Binary Buildpack 的使用</h4><p>Binary Buildpack不想Cloud Foundry的其他Buildpack包，在使用的时候，必须指定Buildpack包。可以使用<code>cf push</code> 的 <code>- b</code>选项来指定相应的本地Buildpack的名字或者github的Buildpack包地址。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ cf push my_app -b https://github.com/cloudfoundry/binary-buildpack.git</div></pre></td></tr></table></figure>
<p>可以通过以下两种途径，执行binary文件：</p>
<ul>
<li><p><strong>Procfile:</strong> In the root directory of your app, add a <code>Procfile</code> that specifies a <code>web</code> task:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">web: ./app</div></pre></td></tr></table></figure>
</li>
<li><p><strong>Command line</strong>: Use <code>cf push APP-NAME</code> with the <code>- c</code> option:</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ cf push my_app -c &apos;./app&apos; -b binary-buildpack</div></pre></td></tr></table></figure>
<p><strong>Compiling your Binary</strong></p>
<p>Your binary should run without any additional runtime dependencies on the <code>cflinuxfs2</code> or <code>lucid64 root filesystem (rootfs)</code>. Any such dependencies should be statically linked to the binary.</p>
<p>To boot a docker container running the cflinuxfs2 filesystem, run the following command:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ docker run -it cloudfoundry/cflinuxfs2 bash</div></pre></td></tr></table></figure>
<p>To boot a docker container running the lucid64 filesystem, run the following command:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ docker run -it cloudfoundry/lucid64 bash</div></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>需要提前安装docker，并配置好docker源</li>
<li>下载比较缓慢，经常失败</li>
</ul>
</blockquote>
<p>When deploying your binary to Cloud Foundry, use cf push with the -s option to specify the root filesystem it should run against.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ cf push my_app -s (cflinuxfs2|lucid64)</div></pre></td></tr></table></figure>
<h4 id="§-Go-Buildpack-的使用"><a href="#§-Go-Buildpack-的使用" class="headerlink" title="§ Go Buildpack 的使用"></a>§ Go Buildpack 的使用</h4><p>The Go buildpack will be automatically detected if:</p>
<ul>
<li>Your app has been packaged with <code>godep</code> using <code>godep save</code></li>
<li>Your app has a <code>vendor/</code> directory and has any files ending with <code>.go</code>.</li>
<li>Your app has a <code>GOPACKAGENAME</code> environment variable specified and has any files ending with <code>.go</code>.</li>
<li>Your app has a <code>glide.yaml</code> file and is using <code>glide</code> (starting in buildpack version 1.7.9).</li>
</ul>
<p><strong>Pushing Apps with godep</strong></p>
<p>If you are using <code>godep</code> to package your dependencies, make sure that you have created a valid <code>Godeps/Godeps.json</code> file in the root directory of your app by running godep save.</p>
<p>When using godep, you can fix your <code>Go version</code> in GoVersion key of the <code>Godeps/Godeps.json</code> file.</p>
<p>Go 1.5<br>An example <code>Godeps/Godeps.json</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;ImportPath&quot;: &quot;go_app&quot;,</div><div class="line">    &quot;GoVersion&quot;: &quot;go1.5&quot;,</div><div class="line">    &quot;Deps&quot;: []</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>An example <code>manifest.yml</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">---</div><div class="line">applications:</div><div class="line">  - name: my-app-name</div></pre></td></tr></table></figure>
<p>Go 1.6</p>
<blockquote>
<p><strong><em>NOTE:</em></strong> if you are using godep with Go 1.6, you must set the <code>GO15VENDOREXPERIMENT</code> environment variable to 0, otherwise your app will not stage.</p>
</blockquote>
<p>An example <code>Godeps/Godeps.json</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;ImportPath&quot;: &quot;go_app&quot;,</div><div class="line">    &quot;GoVersion&quot;: &quot;go1.6&quot;,</div><div class="line">    &quot;Deps&quot;: []</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>An example <code>manifest.yml</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">---</div><div class="line">applications:</div><div class="line">  - name: my-app-name</div><div class="line">    env:</div><div class="line">      GO15VENDOREXPERIMENT: 0</div></pre></td></tr></table></figure>
<p><strong>Pushing Apps with Glide</strong></p>
<p>If you are using <code>glide</code> to specify and/or package your dependencies, make sure that you have created a valid <code>glide.yaml</code> file in the root directory of your app by running <code>glide init</code>.</p>
<p>To vendor your dependencies before pushing, run <code>glide install</code>. This will generate a <code>vendor</code> directory and a <code>glide.lock</code>file specifying the latest compatible versions of your dependencies. A <code>glide.lock</code> is not required when deploying a non-vendored app. A <code>glide.lock</code> is required when pushing a vendored app.</p>
<p>An example <code>glide.yaml</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">package: go_app_with_glide</div><div class="line">import:</div><div class="line">- package: github.com/ZiCog/shiny-thing</div><div class="line">  subpackages:</div><div class="line">  - foo</div></pre></td></tr></table></figure>
<p><strong>Pushing Apps with Native Go Vendoring</strong></p>
<p>If you are using the native Go vendoring system, which packages all local dependencies in the <code>vendor/</code>directory, you must specify your app’s package name in the <code>GOPACKAGENAME</code> environment variable. An example <code>manifest.yml</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">---</div><div class="line">applications:</div><div class="line">  - name: my-app-name</div><div class="line">    command: go-online</div><div class="line">    env:</div><div class="line">      GOPACKAGENAME: go-online</div></pre></td></tr></table></figure>
<p>Go 1.5</p>
<blockquote>
<p><strong><em>NOTE:</em></strong> For Go 1.5, since native vendoring is turned off by default, you must set the environment variable <code>GO15VENDOREXPERIMENT</code> to <code>1</code> in your <code>manifest.yml</code> to use this feature.</p>
</blockquote>
<p>If you are using the <code>vendor/</code> directory for dependencies, you can set the Go version with the <code>GOVERSION</code> environment variable.</p>
<p>An example <code>manifest.yml</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">---</div><div class="line">applications:</div><div class="line">  - name: my-app-name</div><div class="line">    env:</div><div class="line">      GOVERSION: go1.5</div><div class="line">      GOPACKAGENAME: app-package-name</div><div class="line">      GO15VENDOREXPERIMENT: 1</div></pre></td></tr></table></figure>
<p>Go 1.6</p>
<p>An example <code>manifest.yml</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">---</div><div class="line">applications:</div><div class="line"> - name: my-app-name</div><div class="line">   command: example-project</div><div class="line">   env:</div><div class="line">     GOVERSION: go1.6</div><div class="line">     GOPACKAGENAME: github.com/example-org/example-project</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;一、Cloud-Foundry-简介&quot;&gt;&lt;a href=&quot;#一、Cloud-Foundry-简介&quot; class=&quot;headerlink&quot; title=&quot;一、Cloud Foundry 简介&quot;&gt;&lt;/a&gt;一、Cloud Foundry 简介&lt;/h3&gt;&lt;p&gt;Cloud Foundry是VMware推出的业界第一个开源PaaS云平台，它支持多种框架、语言、运行时环境、云平台及应用服务，使开发人员能够在几秒钟内进行应用程序的部署和扩展，无需担心任何基础架构的问题。同时，它本身是一个基于Ruby on Rails的由多个相对独立的子系统通过消息机制组成的分布式系统，使平台在各层级都可水平扩展，既能在大型数据中心里运行，也能运行在一台桌面电脑中，二者使用相同的代码库。&lt;/p&gt;
&lt;p&gt;作为新一代云应用平台，Cloud Foundry专为私有云计算环境、企业级数据中心和公有云服务提供商所打造。Cloud Foundry云平台可以简化现代应用程序的开发、交付和运行过程，在面对多种公有云和私有云选择、符合业界标准的高效开发框架以及应用基础设施服务时，可以显著提高开发者在云环境中部署和运行应用程序的能力。&lt;/p&gt;
    
    </summary>
    
      <category term="学习" scheme="https://www.hanbert.cn/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Cloud Foundry" scheme="https://www.hanbert.cn/tags/Cloud-Foundry/"/>
    
      <category term="部署" scheme="https://www.hanbert.cn/tags/%E9%83%A8%E7%BD%B2/"/>
    
      <category term="Paas" scheme="https://www.hanbert.cn/tags/Paas/"/>
    
  </entry>
  
  <entry>
    <title>通过rvm安装Ruby、Bundler和Rails</title>
    <link href="https://www.hanbert.cn/2016/06/10/%E9%80%9A%E8%BF%87rvm%E5%AE%89%E8%A3%85Ruby%E3%80%81Bundler%E5%92%8CRails/"/>
    <id>https://www.hanbert.cn/2016/06/10/通过rvm安装Ruby、Bundler和Rails/</id>
    <published>2016-06-10T00:23:07.000Z</published>
    <updated>2017-04-10T01:46:42.216Z</updated>
    
    <content type="html"><![CDATA[<p>Ruby Version Manager简称RVM,是一款非常好用的ruby版本管理以及安装工具。</p>
<a id="more"></a>
<h3 id="Step1-安装rvm"><a href="#Step1-安装rvm" class="headerlink" title="Step1: 安装rvm"></a>Step1: 安装rvm</h3><pre><code>$ gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3
$ curl -sSL https://get.rvm.io | bash -s stable
# 如果上面的连接失败，可以尝试: 
$ curl -L https://raw.githubusercontent.com/wayneeseguin/rvm/master/binscripts/rvm-installer | bash -s stable
</code></pre><p>然后，载入 RVM 环境（新开 Termal 就不用这么做了，会自动重新载入)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ source /usr/local/rvm/scripts/rvm</div></pre></td></tr></table></figure>
<p>修改 RVM 下载 Ruby 的源，到 Ruby China 的镜像:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo &quot;ruby_url=https://cache.ruby-china.org/pub/ruby&quot; &gt; /usr/local/rvm/user/db</div></pre></td></tr></table></figure></p>
<h3 id="Step2-用-RVM-安装-Ruby-环境"><a href="#Step2-用-RVM-安装-Ruby-环境" class="headerlink" title="Step2:  用 RVM 安装 Ruby 环境"></a>Step2:  用 RVM 安装 Ruby 环境</h3><p>安装依赖环境：</p>
<pre><code>$ rvm requirements
</code></pre><p>显示已安装的Ruby的列表：</p>
<pre><code>$ rvm list
</code></pre><p>显示可以安装的Ruby的列表：</p>
<pre><code>$ rvm list known
</code></pre><p>安装Ruby 2.3.0：</p>
<pre><code>$ rvm install 2.3.0
</code></pre><p>RVM 装好以后，需要执行下面的命令将指定版本的 Ruby 设置为系统默认版本：</p>
<pre><code>$ rvm use 2.3.0 --default
</code></pre><p>测试是否正确：</p>
<pre><code>$ ruby -v
ruby 2.3.0 ...

$ gem -v
   2.1.6
</code></pre><p>请尽可能用比较新的 RubyGems 版本，建议 2.6.x 以上：</p>
<pre><code>$ gem update --system # 这里请翻墙一下
$ gem -v
2.6.6
</code></pre><p>更换RubyGems的源：</p>
<pre><code>$ gem sources --add https://gems.ruby-china.org/ --remove https://rubygems.org/
$ gem sources -l
  https://gems.ruby-china.org
# 确保只有 gems.ruby-china.org
</code></pre><h3 id="Step3：安装Bundler"><a href="#Step3：安装Bundler" class="headerlink" title="Step3：安装Bundler"></a>Step3：安装Bundler</h3><pre><code>gem install bundler
</code></pre><p>在已有的project的文件下的Gemfile文件中添加以下内容，指定依赖：</p>
<pre><code>source &apos;https://rubygems.org&apos;
gem &apos;nokogiri&apos;
gem &apos;rack&apos;, &apos;~&gt;1.1&apos;
gem &apos;rspec&apos;, :require =&gt; &apos;spec&apos;
</code></pre><p>在同样的文件夹下：</p>
<pre><code>$ bundle install
$ git add Gemfile Gemfile.lock
</code></pre><h3 id="Step4：-安装Rails环境"><a href="#Step4：-安装Rails环境" class="headerlink" title="Step4： 安装Rails环境"></a>Step4： 安装Rails环境</h3><p>Ruby 环境就安装好了，接下来安装 Rails：</p>
<pre><code>$ gem install rails
</code></pre><p>然后测试安装是否正确：</p>
<pre><code>$ rails -v
Rails 4.2.5
</code></pre><blockquote>
<p><strong>More：</strong><br><em><a href="http://bundler.io/" target="_blank" rel="external">http://bundler.io/</a></em><br><em><a href="https://gems.ruby-china.org/" target="_blank" rel="external">https://gems.ruby-china.org/</a></em><br><em><a href="https://ruby-china.org/wiki/install_ruby_guide" target="_blank" rel="external">https://ruby-china.org/wiki/install_ruby_guide</a></em></p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Ruby Version Manager简称RVM,是一款非常好用的ruby版本管理以及安装工具。&lt;/p&gt;
    
    </summary>
    
      <category term="学习" scheme="https://www.hanbert.cn/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="开发环境" scheme="https://www.hanbert.cn/tags/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/"/>
    
      <category term="Ruby" scheme="https://www.hanbert.cn/tags/Ruby/"/>
    
      <category term="Rails" scheme="https://www.hanbert.cn/tags/Rails/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://www.hanbert.cn/2016/04/10/hello-world/"/>
    <id>https://www.hanbert.cn/2016/04/10/hello-world/</id>
    <published>2016-04-10T01:12:33.000Z</published>
    <updated>2017-04-10T01:41:08.754Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<a id="more"></a>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="杂记" scheme="https://www.hanbert.cn/categories/%E6%9D%82%E8%AE%B0/"/>
    
    
      <category term="示例" scheme="https://www.hanbert.cn/tags/%E7%A4%BA%E4%BE%8B/"/>
    
  </entry>
  
</feed>
